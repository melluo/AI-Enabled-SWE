{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs (Solution)\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for the ADR generation lab. It demonstrates how to use an LLM for comparative research and then synthesize that research into a structured, formal document.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks the LLM to generate a standard markdown template for an ADR. The key is to be specific about the sections required (`Title`, `Status`, `Context`, `Decision`, `Consequences`), which guides the LLM to produce a well-structured and reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "```markdown\n",
      "# Title: [A short, descriptive title for the decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "- [Describe the problem, the driving forces, and the constraints.]\n",
      "\n",
      "## Decision\n",
      "- [State the chosen solution clearly and concisely.]\n",
      "\n",
      "## Consequences\n",
      "- [List the positive outcomes, negative trade-offs, and any future work required.]\n",
      "```\n",
      "\n",
      "### Usage Guidelines:\n",
      "- Ensure the **Title** is specific enough to identify the decision at a glance.\n",
      "- **Status** should reflect the current state of the decision, updating it as necessary throughout the decision's lifecycle.\n",
      "- In the **Context** section, provide enough background information for someone unfamiliar with the situation to understand why this decision is necessary.\n",
      "- The **Decision** section should be straightforward, detailing the exact course of action chosen.\n",
      "- Use the **Consequences** section to outline both the expected benefits and potential drawbacks, as well as any additional steps that may be required after implementation.\n",
      "✅ Successfully saved artifact to: templates/adr_template.md\n"
     ]
    }
   ],
   "source": [
    "adr_template_prompt = \"\"\"You are a principal engineer who champions clear documentation. Generate a concise, reusable markdown template for an Architectural Decision Record (ADR).\n",
    "\n",
    "The template must include the following sections:\n",
    "- # Title: [A short, descriptive title for the decision]\n",
    "- **Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
    "- ## Context\n",
    "  - [Describe the problem, the driving forces, and the constraints.]\n",
    "- ## Decision\n",
    "  - [State the chosen solution clearly and concisely.]\n",
    "- ## Consequences\n",
    "  - [List the positive outcomes, negative trade-offs, and any future work required.]\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "adr_template_content = get_completion(adr_template_prompt, client, model_name, api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Explanation:**\n",
    "This prompt leverages the LLM's vast training data to perform a comparative analysis. By instructing it to be an \"unbiased research assistant\" and asking for \"pros and cons for each,\" we guide the model to provide a balanced view rather than a simple recommendation. This produces a more valuable and objective input for our own decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "When considering a semantic search feature for a new hire onboarding tool, the choice between using PostgreSQL with the `pgvector` extension and a specialized vector database like ChromaDB, FAISS, or Weaviate involves several trade-offs. Here's a balanced comparison of both approaches:\n",
      "\n",
      "### Approach 1: PostgreSQL with `pgvector` Extension\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Integration and Familiarity:**\n",
      "   - PostgreSQL is a well-established relational database system. Many development teams are already familiar with its operations, which can reduce the initial learning curve and integration time.\n",
      "\n",
      "2. **Operational Simplicity:**\n",
      "   - Using `pgvector` within PostgreSQL allows teams to leverage their existing database infrastructure without the need to manage a separate system for vector storage and search.\n",
      "\n",
      "3. **Cost-Effectiveness:**\n",
      "   - For teams already using PostgreSQL, adding `pgvector` might incur minimal additional costs compared to adopting a new technology stack.\n",
      "\n",
      "4. **Transactional Support:**\n",
      "   - PostgreSQL provides strong ACID compliance, which may be beneficial if transactional integrity is important alongside vector operations.\n",
      "\n",
      "5. **Query Flexibility:**\n",
      "   - PostgreSQL's robust querying capabilities can be combined with vector operations, allowing for complex queries that integrate semantic search with traditional relational data.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Performance Limitations:**\n",
      "   - While `pgvector` provides vector search capabilities, it may not be as optimized for high-performance vector search as specialized vector databases, particularly for very large datasets.\n",
      "\n",
      "2. **Scalability Constraints:**\n",
      "   - PostgreSQL may face scalability issues when dealing with very large vector datasets or when high throughput is required, as it is primarily optimized for OLTP workloads.\n",
      "\n",
      "3. **Feature Limitations:**\n",
      "   - `pgvector` may not support some advanced features of dedicated vector databases, such as certain types of indexing and search algorithms optimized for high-dimensional data.\n",
      "\n",
      "### Approach 2: Specialized Vector Database (e.g., ChromaDB, FAISS, Weaviate)\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Optimized Performance:**\n",
      "   - Specialized vector databases are designed specifically for vector similarity search, offering faster and more efficient search capabilities, particularly for large datasets and high-dimensional vectors.\n",
      "\n",
      "2. **Advanced Features:**\n",
      "   - These databases often provide advanced indexing techniques and algorithms tailored for vector operations, such as approximate nearest neighbor (ANN) search, which can significantly improve search speed.\n",
      "\n",
      "3. **Scalability:**\n",
      "   - Dedicated vector databases are built to handle large volumes of vector data and can scale horizontally, making them suitable for applications with growing datasets.\n",
      "\n",
      "4. **Community and Ecosystem:**\n",
      "   - Many specialized vector databases have active communities and ecosystems, providing plugins, integrations, and support for machine learning workflows.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Operational Complexity:**\n",
      "   - Managing a separate vector database adds operational overhead, including setup, maintenance, and monitoring, which can increase complexity for the development and operations teams.\n",
      "\n",
      "2. **Cost:**\n",
      "   - Introducing an additional database system can result in higher infrastructure and operational costs, especially if the current system is sufficient for the enterprise's needs.\n",
      "\n",
      "3. **Integration Overhead:**\n",
      "   - Teams need to integrate and possibly synchronize data between the vector database and other systems, which can add development and maintenance burden.\n",
      "\n",
      "4. **Learning Curve:**\n",
      "   - Development teams may need to invest time to learn the nuances of a new database system, which can delay implementation and affect productivity initially.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "For a small-to-medium sized enterprise application, the choice between PostgreSQL with `pgvector` and a specialized vector database largely depends on the specific needs and constraints of the organization. If the team prioritizes operational simplicity and cost-effectiveness, and the vector search workload is moderate, PostgreSQL with `pgvector` might be sufficient. On the other hand, if the application demands high-performance vector search and is expected to scale significantly, investing in a specialized vector database could be more advantageous in the long run.\n"
     ]
    }
   ],
   "source": [
    "db_research_prompt = \"\"\"You are an unbiased research assistant. Your task is to provide a balanced technical comparison for a software development team.\n",
    "\n",
    "For the use case of a new hire onboarding tool that needs a semantic search feature, compare and contrast the following two approaches:\n",
    "\n",
    "1.  **Approach 1:** Using PostgreSQL with the `pgvector` extension.\n",
    "2.  **Approach 2:** Using a specialized, dedicated vector database (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "Please provide a summary of the pros and cons for each approach. Consider factors like operational complexity, cost, query flexibility, and scalability for a small-to-medium sized enterprise application.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "db_research_output = get_completion(db_research_prompt, client, model_name, api_provider)\n",
    "print(db_research_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Explanation:**\n",
    "This prompt demonstrates a powerful synthesis task. We provide the LLM with two key inputs: unstructured information (the research) and a desired structure (the template). The agent's job is to merge them, creating a polished, formal document. This is a repeatable pattern for turning raw analysis into professional documentation. By assigning the persona of a Staff Engineer, we guide the LLM to adopt a formal and authoritative tone suitable for an official project artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "```markdown\n",
      "# Title: Adoption of PostgreSQL with `pgvector` Extension for Semantic Search\n",
      "\n",
      "**Status:** Proposed\n",
      "\n",
      "## Context\n",
      "- The project involves developing a new hire onboarding tool that requires a semantic search feature. The decision to use PostgreSQL with the `pgvector` extension over specialized vector databases such as ChromaDB, FAISS, or Weaviate involves several considerations.\n",
      "- PostgreSQL is a well-established relational database system, familiar to many development teams, which can reduce the initial learning curve and integration time.\n",
      "- Leveraging PostgreSQL with `pgvector` allows the use of existing database infrastructure, minimizing operational complexity by avoiding the need to manage a separate system for vector storage and search.\n",
      "- Cost-effectiveness is another important factor, as integrating `pgvector` into a current PostgreSQL setup may incur fewer additional costs than adopting a new technology stack.\n",
      "- The decision balances the need for transactional integrity and robust querying capabilities provided by PostgreSQL against the high-performance and scalability offered by specialized vector databases.\n",
      "- Constraints include the potential performance limitations and scalability challenges of PostgreSQL when handling very large datasets or high throughput demands, as well as feature limitations compared to specialized vector databases.\n",
      "\n",
      "## Decision\n",
      "- The project will adopt PostgreSQL with the `pgvector` extension to implement the semantic search feature for the new hire onboarding tool.\n",
      "\n",
      "## Consequences\n",
      "- **Positive Outcomes:**\n",
      "  - Reduced learning curve and integration time due to team familiarity with PostgreSQL.\n",
      "  - Simplified operations by utilizing the existing database infrastructure.\n",
      "  - Cost savings from avoiding the introduction of a new technology stack.\n",
      "  - Ability to combine vector operations with traditional relational data queries.\n",
      "  - Maintained transactional integrity with PostgreSQL's ACID compliance.\n",
      "\n",
      "- **Negative Trade-offs:**\n",
      "  - Potential performance and scalability limitations, especially with very large datasets or high throughput requirements.\n",
      "  - Possible lack of advanced features found in specialized vector databases, such as certain indexing techniques and optimized search algorithms for high-dimensional data.\n",
      "\n",
      "- **Future Work:**\n",
      "  - Monitor the system's performance and scalability as the dataset grows to determine if a transition to a specialized vector database becomes necessary.\n",
      "  - Evaluate the need for potential architectural adjustments to accommodate high-dimensional vector data and improve search performance if required.\n",
      "```\n",
      "✅ Successfully saved artifact to: artifacts/adr_001_database_choice.md\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "synthesis_prompt = f\"\"\"You are a Staff Engineer responsible for documenting key architectural decisions.\n",
    "\n",
    "Your task is to populate the provided ADR template to formally document the decision to **use PostgreSQL with the pgvector extension** for our project.\n",
    "\n",
    "Use the research provided below to fill in the 'Context' and 'Consequences' sections of the template. Be thorough and objective, summarizing the key points from the research.\n",
    "\n",
    "--- ADR TEMPLATE ---\n",
    "{adr_template}\n",
    "--- END TEMPLATE ---\n",
    "\n",
    "--- RESEARCH CONTEXT ---\n",
    "{db_research_output}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "The final ADR should be complete and ready for review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    final_adr = get_completion(synthesis_prompt, client, model_name, api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"artifacts/adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
