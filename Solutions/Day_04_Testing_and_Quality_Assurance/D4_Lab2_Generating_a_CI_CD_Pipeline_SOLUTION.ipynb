{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline (Solution)\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts for generating the `requirements.txt`, `Dockerfile`, and GitHub Actions workflow files. It demonstrates how to prompt for specific, structured configuration-as-code artifacts.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Explanation:**\n",
    "This prompt is an analysis task. We provide the application's source code as context and ask the LLM to parse the import statements to identify external libraries. This is much faster and less error-prone than manually creating the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_prompt = f\"\"\"You are a Python dependency analysis tool. Analyze the following Python code and generate a `requirements.txt` file listing all the external libraries imported.\n",
    "\n",
    "Include versions for key libraries like fastapi, uvicorn, sqlalchemy, and pydantic. Also include pytest for running tests.\n",
    "\n",
    "--- PYTHON CODE ---\n",
    "{app_code}\n",
    "--- END CODE ---\n",
    "\n",
    "Output only the contents of the requirements.txt file.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks for a specific, best-practice `Dockerfile`. By explicitly requesting a \"multi-stage\" build, a \"slim\" base image, and a non-root user, we guide the LLM to generate a configuration that is both efficient (smaller final image size) and secure, saving the developer from having to remember these important details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_prompt = \"\"\"You are a DevOps expert specializing in containerization. Generate a best-practice, multi-stage `Dockerfile` for a production Python FastAPI application.\n",
    "\n",
    "The Dockerfile must:\n",
    "1.  Use `python:3.11-slim` as the base image.\n",
    "2.  The first stage should install dependencies from `requirements.txt`.\n",
    "3.  The final stage should copy the application code and the installed dependencies from the first stage.\n",
    "4.  Expose port 8000.\n",
    "5.  The final `CMD` should run the application using `uvicorn`, binding to host 0.0.0.0.\n",
    "\n",
    "Output only the raw Dockerfile content.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(cleaned_dockerfile)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Explanation:**\n",
    "This prompt generates a complete CI workflow in YAML format. Defining our CI pipeline in a YAML file (`ci.yml`) is a core DevOps practice known as 'Configuration as Code.' It makes our build and test process version-controlled, repeatable, and easy to review, just like our application code. We are very specific about the required structure: the `on` trigger, the `jobs` definition, and the `steps` within the job. This level of detail ensures the LLM generates a syntactically correct and logically complete workflow file that is ready to be committed to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_workflow_prompt = \"\"\"You are a CI/CD specialist. Generate a complete GitHub Actions workflow file named `ci.yml` for a Python FastAPI project.\n",
    "\n",
    "The workflow must:\n",
    "- Be named 'Build and Test'.\n",
    "- Trigger on every `push` to the `main` branch.\n",
    "- Define one job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "- The job must have the following sequential steps:\n",
    "  1. `actions/checkout@v4` to check out the repository code.\n",
    "  2. `actions/setup-python@v5` to set up Python 3.11.\n",
    "  3. A step to install dependencies using pip from `requirements.txt`.\n",
    "  4. A step to run the test suite using `pytest`.\n",
    "\n",
    "Output only the raw YAML content for the file.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}