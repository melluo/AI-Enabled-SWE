{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance (Solution)\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for generating a robust test suite. It covers generating simple tests, brainstorming edge cases, and creating the necessary fixtures for professional-grade database testing.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the application's source code to provide the LLM with the necessary context to write accurate tests. A good prompt for test generation should always include the code that needs to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks for the most straightforward type of test: one that verifies the application works as expected when given valid input. We specifically ask for tests for the `POST` and `GET` endpoints. The prompt includes the full application code as context, which is crucial for the LLM to understand the API's structure, expected payloads, and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are a Senior QA Engineer writing tests for a FastAPI application using pytest.\n",
    "\n",
    "Based on the application code provided below, please generate two 'happy path' test functions in a single Python script:\n",
    "1. A test named `test_create_user` for the `POST /users/` endpoint. It should create a user and assert that the status code is 200 and the response email matches the input.\n",
    "2. A test named `test_read_users` for the `GET /users/` endpoint. It should first create a user and then assert the status code is 200 and that the response is a list containing at least one user.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Your response should be only the raw Python code for the tests, including necessary imports like `TestClient` from `fastapi.testclient`.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Explanation:**\n",
    "Good testing goes beyond the happy path. This prompt asks the LLM to think about what could go wrong. We specifically request tests for two common failure modes: creating a duplicate resource (which should be disallowed) and requesting a resource that doesn't exist. This demonstrates how AI can be used as a creative partner to brainstorm potential failure points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer focused on identifying edge cases.\n",
    "\n",
    "Based on the FastAPI application code provided, write two test functions for common error scenarios:\n",
    "1.  A test named `test_create_user_duplicate_email` that attempts to create a user with an email that already exists. It must assert that the API returns a 400 status code.\n",
    "2.  A test named `test_read_user_not_found` that attempts to GET a user with an ID that does not exist (e.g., 999). It must assert that the API returns a 404 status code.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for these two test functions.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "    # In a real scenario, you'd append these to your test file.\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Explanation:**\n",
    "This is the most advanced and most important concept in this lab. A `pytest` fixture is a function that runs before each test to set up a specific state or resource. \n",
    "\n",
    "The prompt asks the LLM to generate a fixture that creates an isolated, in-memory database for testing. This is a best practice because it ensures tests are independent and don't interfere with each other or with the real development database. \n",
    "\n",
    "We specifically instruct the LLM to save this fixture in `tests/conftest.py`. `conftest.py` is a special file that pytest automatically discovers. Fixtures defined here are globally available to all test files in the same directory and subdirectories, making it the ideal place to put shared setup code like a database connection. \n",
    "\n",
    "Finally, we ask the LLM to refactor our original happy-path tests to *use* this fixture by simply adding it as a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_fixture_prompt = f\"\"\"\n",
    "You are an expert in Python testing with pytest and FastAPI.\n",
    "\n",
    "I need to create a `pytest` fixture to provide an isolated, in-memory SQLite database session for each test run. This is a critical best practice for testing database-connected applications.\n",
    "\n",
    "Please generate the Python code for a file named `tests/conftest.py` that contains this fixture.\n",
    "\n",
    "The fixture should:\n",
    "1. Be named `db_session`.\n",
    "2. Configure a SQLAlchemy engine for an in-memory SQLite database.\n",
    "3. Create all database tables before the tests run.\n",
    "4. Yield a database session.\n",
    "5. Clean up the database tables after the tests are complete.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for the `conftest.py` file.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture for conftest.py ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer refactoring a test suite to use a new database fixture.\n",
    "\n",
    "Given the following tests and the knowledge that a fixture named `db_session` and a `TestClient` instance named `client` are now available from `conftest.py`, please rewrite the tests to use them. The tests should no longer create their own client instances.\n",
    "\n",
    "**Original Tests:**\n",
    "```python\n",
    "{generated_happy_path_tests}\n",
    "```\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for the refactored tests.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests for test_main_with_fixture.py ---\")\n",
    "if app_code and 'generated_happy_path_tests' in locals():\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context or original tests are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}