{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7 - Lab 2: Agent Interoperability with A2A Protocol (Solution)\n\n",
    "**Objective:** To provide students with hands-on experience implementing the A2A Protocol, enabling them to build two distinct agents that can discover and communicate with each other in a standardized way.\n\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete code for the A2A Protocol lab. It includes the full implementations for both the Responder and Requester agents, and demonstrates how to wrap the A2A client into a LangChain tool for use by a higher-level reasoning agent.\n\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('a2a_protocol')\n",
    "\n",
    "from utils import setup_llm_client, save_artifact\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Implementing the Responder Agent\n\n",
    "**Explanation:**\n",
    "This script creates the 'server' agent. \n",
    "1.  We define a standard Python function `calculate_task_complexity`.\n",
    "2.  The `Service` object from the A2A SDK wraps this function, exposing its signature (name, arguments, types) as a discoverable service.\n",
    "3.  The `Responder` is the main agent process. We give it a unique name for discovery and register our service with it.\n",
    "4.  `responder.start()` begins listening on a local port for incoming A2A protocol messages. The `while True` loop keeps the script running to serve requests."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "responder_code = \"\"\"# a2a_responder.py\n",
    "import time\n",
    "from a2a_protocol import Responder, Service\n",
    "\n",
    "# 1. Define the function that will be our service.\n",
    "def calculate_task_complexity(steps: int, priority: int) -> str:\n",
    "    \"\"\"Calculates a simple complexity score based on steps and priority.\"\"\"\n",
    "    if not isinstance(steps, int) or not isinstance(priority, int):\n",
    "        return 'Error: Both steps and priority must be integers.'\n",
    "    complexity = steps * priority\n",
    "    return f'The calculated complexity score is {complexity}.'\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing Responder Agent...\")\n",
    "    \n",
    "    # 2. Create a Service object from the function.\n",
    "    complexity_service = Service(name=\"calculate_task_complexity\", func=calculate_task_complexity)\n",
    "    \n",
    "    # 3. Instantiate the Responder.\n",
    "    responder = Responder(name=\"ComplexityCalculatorAgent\", services=[complexity_service])\n",
    "    \n",
    "    try:\n",
    "        # 4. Start the responder.\n",
    "        responder.start()\n",
    "        print(f\"Responder '{responder.name}' started at {responder.address}\")\n",
    "        print(\"Waiting for requests... (Press Ctrl+C to stop)\")\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nShutting down responder...\")\n",
    "        responder.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "save_artifact(responder_code, \"a2a_responder.py\")\n",
    "print(\"Saved 'a2a_responder.py'. Run it in a separate terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Implementing the Requester Agent\n\n",
    "**Explanation:**\n",
    "This script creates the 'client' agent.\n",
    "1.  `Requester()` creates our client agent.\n",
    "2.  `requester.discover_responders()` sends out a broadcast on the local network to find any running A2A Responders.\n",
    "3.  `responder_proxy.discover_services()` connects to a specific responder and asks for its list of available services.\n",
    "4.  `responder_proxy.invoke(...)` calls the specific service by name, passing the arguments as keyword arguments. The A2A protocol handles the serialization, network communication, and response deserialization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "requester_code = \"\"\"# a2a_requester.py\n",
    "import asyncio\n",
    "from a2a_protocol import Requester\n",
    "\n",
    "async def main():\n",
    "    print(\"Initializing Requester Agent...\")\n",
    "    # 1. Instantiate the Requester\n",
    "    requester = Requester()\n",
    "\n",
    "    print(\"Discovering responders on the network...\")\n",
    "    # 2. Discover responders. This may take a few seconds.\n",
    "    responders = await requester.discover_responders(timeout=5)\n",
    "\n",
    "    if not responders:\n",
    "        print(\"No responders found. Is the a2a_responder.py script running?\")\n",
    "        return\n",
    "\n",
    "    # Connect to the first responder found\n",
    "    responder_proxy = responders[0]\n",
    "    print(f\"Connected to responder: {responder_proxy.name}\")\n",
    "\n",
    "    # 3. Discover the services offered by the responder.\n",
    "    services = await responder_proxy.discover_services()\n",
    "    print(f\"Discovered services: {services}\")\n",
    "\n",
    "    if 'calculate_task_complexity' in services:\n",
    "        print(\"\\nInvoking 'calculate_task_complexity' service...\")\n",
    "        # 4. Invoke the service with keyword arguments.\n",
    "        result = await responder_proxy.invoke('calculate_task_complexity', steps=10, priority=3)\n",
    "        print(f\"Service responded with: {result}\")\n",
    "    else:\n",
    "        print(\"The required service was not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\"\"\"\n",
    "\n",
    "save_artifact(requester_code, \"a2a_requester.py\")\n",
    "print(\"Saved 'a2a_requester.py'. Run it in a second terminal while the responder is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Integrating A2A as a LangChain Tool\n\n",
    "**Explanation:**\n",
    "This is the most powerful part of the lab. We abstract the low-level A2A communication behind a simple LangChain tool. \n",
    "1.  The `@tool` decorator turns our `get_task_complexity` function into a tool the LangChain agent can see.\n",
    "2.  The docstring `\"Calculates the complexity of a task...\"` is critical. This is the *only* information the LLM has about what the tool does. A good docstring is essential for the agent to know when to use the tool.\n",
    "3.  When the `AgentExecutor` is invoked with a natural language query, the LLM reasons that it needs to calculate complexity, sees our tool is the best fit, and invokes it. \n",
    "4.  Our tool's code then runs, performing the A2A communication with the Responder agent to get the final answer, which is then passed back to the LLM to be formatted for the user."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from a2a_protocol import Requester\n",
    "import asyncio\n",
    "\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "\n",
    "@tool\n",
    "async def get_task_complexity(steps: int, priority: int) -> str:\n",
    "    \"\"\"Calculates the complexity of a task based on the number of steps and its priority level. Use this for any questions about task complexity.\"\"\"\n",
    "    requester = Requester()\n",
    "    responders = await requester.discover_responders(timeout=5)\n",
    "    if not responders:\n",
    "        return \"Error: Could not find the ComplexityCalculatorAgent.\"\n",
    "    \n",
    "    responder_proxy = responders[0]\n",
    "    try:\n",
    "        result = await responder_proxy.invoke('calculate_task_complexity', steps=steps, priority=priority)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error invoking service: {e}\"\n",
    "\n",
    "# Create the LangChain agent\n",
    "tools = [get_task_complexity]\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# This function is needed to run the async invoke in a sync notebook environment\n",
    "async def run_agent_query():\n",
    "    print(\"--- Invoking LangChain Agent with A2A Tool ---\")\n",
    "    # NOTE: Ensure your a2a_responder.py is running in a separate terminal before executing this cell.\n",
    "    result = await agent_executor.ainvoke({\"input\": \"How complex is a task with 8 steps and a priority level of 5?\"})\n",
    "    print(f\"\\nFinal Answer: {result['output']}\")\n",
    "\n",
    "# Run the async function\n",
    "# asyncio.run(run_agent_query())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n\n",
    "Excellent work! You have successfully implemented the Agent2Agent protocol, creating two distinct agents that can communicate in a standardized way. More importantly, you integrated this low-level communication into a high-level LangChain agent, demonstrating how specialized, protocol-driven agents can be exposed as tools for more general-purpose reasoning agents. This is a key architectural pattern for building complex, interoperable multi-agent systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
