{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 5 - Lab 1: Foundations of AI Agents - Tool-Using Agents (Solution)\n",
        "\n",
        "**Objective:** Introduce the fundamental concepts of AI agents by building agents that can use external tools to accomplish tasks they cannot perform on their own.\n",
        "\n",
        "**Introduction:**\n",
        "This solution notebook provides the complete code and explanations for building your first agents. It covers the OpenAI Assistants API for a high-level introduction and then dives into the LangChain framework for more control and flexibility, culminating in a multi-tool agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "**Explanation:**\n",
        "We ensure all necessary libraries for this lab are installed. `langchain` and its related packages provide the core framework for building agents, while `tavily-python` is the SDK for the Tavily Search API, which our agent will use as a tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project's root directory to the Python path\n",
        "try:\n",
        "    # This works when running as a script\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "except NameError:\n",
        "    # This works when running in an interactive environment (like a notebook)\n",
        "    # We go up two levels from the notebook's directory to the project root.\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This helper will install packages if they are not found\n",
        "import importlib\n",
        "def install_if_missing(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "    except ImportError:\n",
        "        print(f\"{package} not found, installing...\")\n",
        "        %pip install -q {package}\n",
        "\n",
        "install_if_missing('langchain')\n",
        "install_if_missing('langchain_community')\n",
        "install_if_missing('langchain_openai')\n",
        "install_if_missing('tavily-python')\n",
        "\n",
        "from utils import setup_llm_client\n",
        "# We will use the OpenAI provider for this lab\n",
        "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Challenges - Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1 (Foundational): Using the OpenAI Assistants API\n",
        "\n",
        "**Explanation:**\n",
        "The Assistants API is OpenAI's high-level framework for building stateful agents. \n",
        "1.  **`client.beta.assistants.create`**: We define the agent's persona and capabilities. We give it instructions and enable the `code_interpreter` tool, which allows it to execute Python code in a sandboxed environment to perform tasks like calculations.\n",
        "2.  **`client.beta.threads.create`**: Assistants are stateful. A `thread` represents a single conversation, allowing the assistant to remember previous messages.\n",
        "3.  **`client.beta.threads.messages.create`**: We add our user's question to the conversation thread.\n",
        "4.  **`client.beta.threads.runs.create`**: A `run` is the process of the assistant executing to respond to the messages in the thread. It's an asynchronous operation, so we poll its status until it's `completed`.\n",
        "5.  **`client.beta.threads.messages.list`**: Once the run is complete, we retrieve the full list of messages from the thread, which now includes the assistant's response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "if api_provider != 'openai':\n",
        "    print(\"This challenge requires an OpenAI client. Please set up your client accordingly.\")\n",
        "else:\n",
        "    # 1. Create an assistant\n",
        "    assistant = client.beta.assistants.create(\n",
        "        name=\"Math Tutor\",\n",
        "        instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
        "        tools=[{\"type\": \"code_interpreter\"}],\n",
        "        model=model_name\n",
        "    )\n",
        "    print(f\"Assistant created with ID: {assistant.id}\")\n",
        "\n",
        "    # 2. Create a thread\n",
        "    thread = client.beta.threads.create()\n",
        "    print(f\"Thread created with ID: {thread.id}\")\n",
        "\n",
        "    # 3. Add a message to the thread\n",
        "    message = client.beta.threads.messages.create(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=\"What is `(123 * 4) + (567 / 8)`?\"\n",
        "    )\n",
        "\n",
        "    # 4. Create a run and wait for completion\n",
        "    run = client.beta.threads.runs.create(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant.id,\n",
        "    )\n",
        "    \n",
        "    while run.status != 'completed':\n",
        "        time.sleep(1)\n",
        "        run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "        print(f\"Run status: {run.status}\")\n",
        "\n",
        "    # 5. Retrieve and print the messages\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "    for msg in reversed(messages.data): # Reverse to show in chronological order\n",
        "        print(f\"\\n{msg.role.capitalize()}: {msg.content[0].text.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2 (Intermediate): Building a LangChain Agent with One Tool\n",
        "\n",
        "**Explanation:**\n",
        "LangChain provides a more modular and flexible way to build agents. \n",
        "1.  **Tool:** We instantiate `TavilySearchResults`, which is a pre-built LangChain component that knows how to call the Tavily API.\n",
        "2.  **Prompt:** The prompt is crucial. `ChatPromptTemplate.from_messages` creates a template for the conversation. The key part is the `(\"placeholder\", \"{agent_scratchpad}\")`. The `agent_scratchpad` is a special variable where the agent's internal monologue (its thoughts, tool calls, and tool outputs) is stored. This allows the agent to reason about its actions.\n",
        "3.  **Agent:** `create_tool_calling_agent` binds the LLM, the tools, and the prompt together into a runnable agent.\n",
        "4.  **AgentExecutor:** This is the runtime for the agent. It takes the agent and the tools and handles the logic of calling the agent, parsing its output to see if it wants to use a tool, executing the tool, and passing the result back to the agent to continue its reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We need to use a LangChain LLM wrapper\n",
        "llm = ChatOpenAI(model=model_name)\n",
        "\n",
        "# 1. Instantiate the Tavily search tool\n",
        "search_tool = TavilySearchResults(max_results=2)\n",
        "tools = [search_tool]\n",
        "\n",
        "# 2. Create the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "# 3. Create the agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "# 4. Create the AgentExecutor\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# 5. Invoke the agent with a question\n",
        "question = \"What was the score of the last Super Bowl?\"\n",
        "result = agent_executor.invoke({\"input\": question})\n",
        "\n",
        "print(f\"\\nFinal Answer: {result['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3 (Advanced): Building a Multi-Tool Agent\n",
        "\n",
        "**Explanation:**\n",
        "This challenge demonstrates the core reasoning power of an agent. \n",
        "1.  **`@tool` Decorator:** LangChain provides a simple `@tool` decorator to turn any Python function into a tool that an agent can use. The function's docstring is very important, as the agent uses it to understand what the tool does.\n",
        "2.  **Tool List:** We create a new list containing both our custom `multiply` tool and the pre-built `TavilySearchResults` tool.\n",
        "3.  **Reasoning:** When we create the new `AgentExecutor` with this list, the agent now has a choice. When invoked, the LLM will look at the user's question and the docstrings of all available tools. It then decides which tool, if any, is the most appropriate for the task. For \"25 * 48\", it will see the \"Multiplies two integers\" docstring and choose the `multiply` tool. For \"Who is the CEO of Apple?\", it will recognize that it needs external information and choose the search tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# 1. Define your custom calculator tool\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies two integers together. Use this for math questions involving multiplication.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# 2. Create the new list of tools\n",
        "multi_tool_list = [TavilySearchResults(max_results=2), multiply]\n",
        "\n",
        "# 3. Create the new multi-tool agent and executor\n",
        "# The prompt is the same, the agent just has more tools to choose from.\n",
        "multi_tool_agent = create_tool_calling_agent(llm, multi_tool_list, prompt)\n",
        "multi_tool_executor = AgentExecutor(agent=multi_tool_agent, tools=multi_tool_list, verbose=True)\n",
        "\n",
        "# 4. Invoke the agent with a math question\n",
        "math_question = \"What is 25 * 48?\"\n",
        "math_result = multi_tool_executor.invoke({\"input\": math_question})\n",
        "print(f\"\\nQuery: {math_question}\\nFinal Answer: {math_result['output']}\\n\")\n",
        "\n",
        "# 5. Invoke the agent with a search question\n",
        "search_question = \"Who is the current CEO of Apple?\"\n",
        "search_result = multi_tool_executor.invoke({\"input\": search_question})\n",
        "print(f\"\\nQuery: {search_question}\\nFinal Answer: {search_result['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Conclusion\n",
        "\n",
        "Congratulations! You have successfully built your first AI agents. You started with the high-level OpenAI Assistants API and then moved to the more flexible LangChain framework. You've learned how to give agents tools to extend their capabilities and, most importantly, how to build an agent that can reason about which tool to use for a specific task. This is the foundational skill for all advanced agentic workflows we will explore in the coming days."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}