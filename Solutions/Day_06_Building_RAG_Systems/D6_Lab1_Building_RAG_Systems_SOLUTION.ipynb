{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems (Solution)\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete code and explanations for building the multi-agent RAG system. It demonstrates how to use LangGraph to create increasingly complex and capable agentic workflows.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "**Explanation:**\n",
    "This function gathers all our project documents, loads them, splits them into manageable chunks, and creates a FAISS vector store. The vector store converts the text chunks into numerical embeddings, which allows for efficient semantic search. The function returns a `retriever` object, which is the component our agents will use to query the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/day1_prd.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Explanation:**\n",
    "This is the simplest form of a LangGraph system. \n",
    "1.  **`AgentState`**: We define the 'state' of our graph using a `TypedDict`. This is the shared memory that all nodes in the graph can read from and write to.\n",
    "2.  **Nodes**: Each node is a Python function that performs an action. The `retrieve` node calls our retriever, and the `generate` node calls the LLM.\n",
    "3.  **Graph Definition**: We instantiate `StateGraph` and add our nodes. The `set_entry_point` and `add_edge` methods define the directed flow of the graph. `compile()` creates the runnable graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---NODE: RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---NODE: GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\nQuestion: {question}\\n\\nContext: {documents}\\n\\nAnswer:\"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "workflow_v1 = StateGraph(SimpleAgentState)\n",
    "workflow_v1.add_node(\"RETRIEVE\", retrieve)\n",
    "workflow_v1.add_node(\"GENERATE\", generate)\n",
    "workflow_v1.set_entry_point(\"RETRIEVE\")\n",
    "workflow_v1.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
    "workflow_v1.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app_v1 = workflow_v1.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Simple RAG Graph ---\")\n",
    "inputs = {\"question\": \"What is the purpose of this project according to the PRD?\"}\n",
    "result = app_v1.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Explanation:**\n",
    "Adding a Grader agent prevents the system from trying to answer a question with irrelevant information. This directly combats hallucination and makes the RAG system more trustworthy by allowing it to gracefully say, 'I don't know,' instead of making something up.\n",
    "\n",
    "1.  **`GraderAgent` Node:** We create a new node whose sole purpose is to act as a 'grader'. It calls the LLM with a very specific prompt, asking for a 'yes' or 'no' answer on whether the retrieved documents are relevant.\n",
    "2.  **Conditional Edge:** This is the key concept. `workflow.add_conditional_edges` tells the graph to execute a function (`decide_to_generate`) after the `GRADE` node. This function checks the output of the grader and returns the name of the *next* node to execute. This allows for dynamic routing and makes the agent much smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraderAgentState(SimpleAgentState):\n",
    "    grade: str\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---NODE: GRADE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. Grade 'yes' or 'no'.\\n\\nRetrieved Document: {documents}\\n\\nUser Question: {question}\"\"\"\n",
    "    grade = llm.invoke(prompt).content\n",
    "    return {\"grade\": grade}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    print(\"---NODE: CONDITIONAL EDGE---\")\n",
    "    if state[\"grade\"].lower() == \"yes\":\n",
    "        print(\"DECISION: Documents are relevant. Proceed to generation.\")\n",
    "        return \"GENERATE\"\n",
    "    else:\n",
    "        print(\"DECISION: Documents are not relevant. End process.\")\n",
    "        return END\n",
    "\n",
    "workflow_v2 = StateGraph(GraderAgentState)\n",
    "workflow_v2.add_node(\"RETRIEVE\", retrieve)\n",
    "workflow_v2.add_node(\"GRADE\", grade_documents)\n",
    "workflow_v2.add_node(\"GENERATE\", generate)\n",
    "\n",
    "workflow_v2.set_entry_point(\"RETRIEVE\")\n",
    "workflow_v2.add_edge(\"RETRIEVE\", \"GRADE\")\n",
    "workflow_v2.add_conditional_edges(\"GRADE\", decide_to_generate)\n",
    "workflow_v2.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app_v2 = workflow_v2.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with a relevant question ---\")\n",
    "inputs = {\"question\": \"What database schema will we use?\"}\n",
    "result = app_v2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with an irrelevant question ---\")\n",
    "inputs = {\"question\": \"What is the weather in Paris?\"}\n",
    "result = app_v2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Explanation:**\n",
    "This is a highly advanced workflow that mimics a real research team.\n",
    "1.  **Specialized Retrievers:** We create two separate vector stores and retrievers. This specialization allows us to direct queries to the most relevant knowledge source.\n",
    "2.  **Router/PM Agent:** The `ProjectManagerAgent` acts as a 'router.' This is a highly efficient pattern. Instead of one giant agent searching through all documents, the router first makes a quick, low-cost decision to delegate the task to a specialized agent with a smaller, more relevant knowledge base. This improves both speed and accuracy.\n",
    "3.  **Graph Construction:** We build the most complex graph yet. The entry point is the router. Based on its decision, the graph flows to one of the two specialist researchers. Both of their paths then converge on the `SYNTHESIZE` node, which creates the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create specialized retrievers\n",
    "prd_retriever = create_knowledge_base([\"artifacts/day1_prd.md\"])\n",
    "tech_retriever = create_knowledge_base([\"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"])\n",
    "\n",
    "class ResearchTeamState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# 2. Define the agent nodes\n",
    "def prd_researcher(state):\n",
    "    print(\"---NODE: PRD RESEARCHER---\")\n",
    "    documents = prd_retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def tech_researcher(state):\n",
    "    print(\"---NODE: TECH RESEARCHER---\")\n",
    "    documents = tech_retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def synthesize_answer(state):\n",
    "    print(\"---NODE: SYNTHESIZE ANSWER---\")\n",
    "    prompt = f\"Based on the following documents, create a concise answer to the user's question.\\n\\nQuestion: {state['question']}\\n\\nDocuments: {state['documents']}\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "def project_manager_router(state):\n",
    "    print(\"---NODE: PROJECT MANAGER (ROUTER)---\")\n",
    "    prompt = f\"You are a project manager. Based on the user's question, should you route this to the PRD expert or the Technical expert? Answer with 'PRD_RESEARCHER' or 'TECH_RESEARCHER'.\\n\\nQuestion: {state['question']}\"\n",
    "    decision = llm.invoke(prompt).content\n",
    "    print(f\"PM Decision: Route to {decision}\")\n",
    "    if 'PRD_RESEARCHER' in decision:\n",
    "        return \"PRD_RESEARCHER\"\n",
    "    else:\n",
    "        return \"TECH_RESEARCHER\"\n",
    "\n",
    "# 3. Build the graph\n",
    "workflow_v3 = StateGraph(ResearchTeamState)\n",
    "workflow_v3.add_node(\"PRD_RESEARCHER\", prd_researcher)\n",
    "workflow_v3.add_node(\"TECH_RESEARCHER\", tech_researcher)\n",
    "workflow_v3.add_node(\"SYNTHESIZE\", synthesize_answer)\n",
    "\n",
    "workflow_v3.add_conditional_edges(\"__start__\", project_manager_router)\n",
    "workflow_v3.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZE\")\n",
    "workflow_v3.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZE\")\n",
    "workflow_v3.add_edge(\"SYNTHESIZE\", END)\n",
    "\n",
    "app_v3 = workflow_v3.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Research Team with a PRD question ---\")\n",
    "inputs = {\"question\": \"What are the main user personas for this application?\"}\n",
    "result = app_v3.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")\n",
    "\n",
    "print(\"\\n--- Invoking Research Team with a technical question ---\")\n",
    "inputs = {\"question\": \"What columns are in the users table?\"}\n",
    "result = app_v3.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}