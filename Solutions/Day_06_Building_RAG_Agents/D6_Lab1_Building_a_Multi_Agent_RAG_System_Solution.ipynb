{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 6 - Lab 1: Building a Multi-Agent RAG System (Solution)\n",
        "\n",
        "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a single agent to a multi-agent team that can reason about a knowledge base.\n",
        "\n",
        "**Introduction:**\n",
        "This solution notebook provides the complete code and explanations for building the multi-agent RAG system. It demonstrates how to use LangGraph to create increasingly complex and capable agentic workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project's root directory to the Python path\n",
        "try:\n",
        "    # This works when running as a script\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "except NameError:\n",
        "    # This works when running in an interactive environment (like a notebook)\n",
        "    # We go up two levels from the notebook's directory to the project root.\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "def install_if_missing(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "    except ImportError:\n",
        "        print(f\"{package} not found, installing...\")\n",
        "        %pip install -q {package}\n",
        "\n",
        "install_if_missing('langgraph')\n",
        "install_if_missing('langchain')\n",
        "install_if_missing('langchain_community')\n",
        "install_if_missing('langchain_openai')\n",
        "install_if_missing('faiss-cpu')\n",
        "install_if_missing('pypdf')\n",
        "\n",
        "import os\n",
        "import operator\n",
        "from typing import TypedDict, Annotated, List\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import BaseMessage\n",
        "from utils import setup_llm_client, load_artifact\n",
        "\n",
        "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
        "llm = ChatOpenAI(model=model_name)\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Building the Knowledge Base\n",
        "\n",
        "**Explanation:**\n",
        "This function gathers all our project documents, loads them, splits them into manageable chunks, and creates a FAISS vector store. The vector store converts the text chunks into numerical embeddings, which allows for efficient semantic search. The function returns a `retriever` object, which is the component our agents will use to query the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_knowledge_base(file_paths):\n",
        "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\"\n",
        "    all_docs = []\n",
        "    for path in file_paths:\n",
        "        if os.path.exists(path):\n",
        "            if path.endswith(\".pdf\"):\n",
        "                loader = PyPDFLoader(path)\n",
        "            else:\n",
        "                loader = TextLoader(path)\n",
        "            docs = loader.load()\n",
        "            for doc in docs:\n",
        "                doc.metadata={\"source\": path} # Add source metadata\n",
        "            all_docs.extend(docs)\n",
        "        else:\n",
        "            print(f\"Warning: Artifact not found at {path}\")\n",
        "\n",
        "    if not all_docs:\n",
        "        print(\"No documents found to create knowledge base.\")\n",
        "        return None\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(all_docs)\n",
        "    \n",
        "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    return vectorstore.as_retriever()\n",
        "\n",
        "all_artifact_paths = [\"artifacts/prd.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_framework_choice.md\"]\n",
        "retriever = create_knowledge_base(all_artifact_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: The Challenges - Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1 (Foundational): A Single-Agent RAG System\n",
        "\n",
        "**Explanation:**\n",
        "This is the simplest form of a LangGraph system. \n",
        "1.  **`AgentState`**: We define the 'state' of our graph using a `TypedDict`. This is the shared memory that all nodes in the graph can read from and write to.\n",
        "2.  **Nodes**: Each node is a Python function that performs an action. The `retrieve_documents` node calls our retriever, and the `generate_answer` node calls the LLM.\n",
        "3.  **Graph Definition**: We instantiate `StateGraph` and add our nodes. The `set_entry_point` and `add_edge` methods define the directed flow of the graph. `compile()` creates the runnable graph object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    documents: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def retrieve_documents(state):\n",
        "    print(\"---NODE: RETRIEVE DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def generate_answer(state):\n",
        "    print(\"---NODE: GENERATE ANSWER---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\nQuestion: {question}\\n\\nContext: {documents}\\n\\nAnswer:\"\"\"\n",
        "    answer = llm.invoke(prompt).content\n",
        "    return {\"answer\": answer}\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"RETRIEVE\", retrieve_documents)\n",
        "workflow.add_node(\"GENERATE\", generate_answer)\n",
        "workflow.set_entry_point(\"RETRIEVE\")\n",
        "workflow.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
        "workflow.add_edge(\"GENERATE\", END)\n",
        "\n",
        "app_v1 = workflow.compile()\n",
        "\n",
        "print(\"\\n--- Invoking Single-Agent Graph ---\")\n",
        "inputs = {\"question\": \"What is the purpose of this project according to the PRD?\"}\n",
        "result = app_v1.invoke(inputs)\n",
        "print(f\"Final Answer: {result['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2 (Intermediate): A Two-Agent System with a Grader\n",
        "\n",
        "**Explanation:**\n",
        "Here, we add a decision-making step. \n",
        "1.  **`GraderAgent` Node:** We create a new node whose sole purpose is to act as a 'grader'. It calls the LLM with a very specific prompt, asking for a 'yes' or 'no' answer on whether the retrieved documents are relevant.\n",
        "2.  **Conditional Edge:** This is the key concept. `workflow.add_conditional_edges` tells the graph to execute a function (`decide_to_generate`) after the `GRADE` node. This function checks the output of the grader and returns the name of the *next* node to execute. This allows for dynamic routing and makes the agent much smarter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraderState(AgentState):\n",
        "    grade: str\n",
        "\n",
        "def grade_documents(state):\n",
        "    print(\"---NODE: GRADE DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. Grade 'yes' or 'no'.\\n\\nRetrieved Document: {documents}\\n\\nUser Question: {question}\"\"\"\n",
        "    grade = llm.invoke(prompt).content\n",
        "    return {\"grade\": grade}\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    print(\"---NODE: CONDITIONAL EDGE---\")\n",
        "    if state[\"grade\"].lower() == \"yes\":\n",
        "        print(\"DECISION: Documents are relevant. Proceed to generation.\")\n",
        "        return \"GENERATE\"\n",
        "    else:\n",
        "        print(\"DECISION: Documents are not relevant. End process.\")\n",
        "        return END\n",
        "\n",
        "workflow_v2 = StateGraph(GraderState)\n",
        "workflow_v2.add_node(\"RETRIEVE\", retrieve_documents)\n",
        "workflow_v2.add_node(\"GRADE\", grade_documents)\n",
        "workflow_v2.add_node(\"GENERATE\", generate_answer)\n",
        "\n",
        "workflow_v2.set_entry_point(\"RETRIEVE\")\n",
        "workflow_v2.add_edge(\"RETRIEVE\", \"GRADE\")\n",
        "workflow_v2.add_conditional_edges(\n",
        "    \"GRADE\",\n",
        "    decide_to_generate,\n",
        ")\n",
        "workflow_v2.add_edge(\"GENERATE\", END)\n",
        "\n",
        "app_v2 = workflow_v2.compile()\n",
        "\n",
        "print(\"\\n--- Invoking Two-Agent Graph with a relevant question ---\")\n",
        "inputs = {\"question\": \"What database schema will we use?\"}\n",
        "result = app_v2.invoke(inputs)\n",
        "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")\n",
        "\n",
        "print(\"\\n--- Invoking Two-Agent Graph with an irrelevant question ---\")\n",
        "inputs = {\"question\": \"What is the weather in Paris?\"}\n",
        "result = app_v2.invoke(inputs)\n",
        "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3 (Advanced): A 5-Agent Research Team with Human-in-the-Loop\n",
        "\n",
        "**Explanation:**\n",
        "This is a highly advanced workflow that mimics a real research team.\n",
        "1.  **Specialized Retrievers:** We create two separate vector stores and retrievers. This specialization allows us to direct queries to the most relevant knowledge source.\n",
        "2.  **Router/PM Agent:** The `project_manager_agent` node acts as a router. It uses the LLM's reasoning ability to decide which specialist (PRD or Tech researcher) should handle the query.\n",
        "3.  **Human-in-the-Loop:** The `human_validation` node is a critical pattern for responsible AI. It explicitly pauses the automated process and requires human confirmation before proceeding. This is essential for tasks where the AI's output could have significant consequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Create specialized retrievers\n",
        "prd_retriever = create_knowledge_base([\"artifacts/prd.md\"])\n",
        "tech_retriever = create_knowledge_base([\"artifacts/schema.sql\", \"artifacts/adr_001_framework_choice.md\"])\n",
        "\n",
        "class ResearchState(TypedDict):\n",
        "    question: str\n",
        "    documents: List[Document]\n",
        "    draft_answer: str\n",
        "    final_answer: str\n",
        "\n",
        "# 2. Define the agent nodes\n",
        "def prd_researcher_agent(state):\n",
        "    print(\"---NODE: PRD RESEARCHER---\")\n",
        "    documents = prd_retriever.invoke(state[\"question\"])\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "def tech_researcher_agent(state):\n",
        "    print(\"---NODE: TECH RESEARCHER---\")\n",
        "    documents = tech_retriever.invoke(state[\"question\"])\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "def synthesizer_agent(state):\n",
        "    print(\"---NODE: SYNTHESIZER---\")\n",
        "    prompt = f\"Based on the following documents, create a concise draft answer to the user's question.\\n\\nQuestion: {state['question']}\\n\\nDocuments: {state['documents']}\"\n",
        "    draft = llm.invoke(prompt).content\n",
        "    return {\"draft_answer\": draft}\n",
        "\n",
        "def human_validation_node(state):\n",
        "    print(\"---NODE: HUMAN VALIDATION---\")\n",
        "    print(f\"Draft Answer: {state['draft_answer']}\")\n",
        "    print(\"Sources:\", [doc.metadata['source'] for doc in state['documents']])\n",
        "    response = input(\"Is this answer helpful and correct? (yes/no): \")\n",
        "    if response.lower() == 'yes':\n",
        "        return {\"final_answer\": state['draft_answer']}\n",
        "    else:\n",
        "        return {\"final_answer\": \"The user rejected the draft answer.\"}\n",
        "\n",
        "def project_manager_router(state):\n",
        "    print(\"---NODE: PROJECT MANAGER (ROUTER)---\")\n",
        "    prompt = f\"You are a project manager. Based on the user's question, should you route this to the PRD expert or the Technical expert? Answer with 'prd' or 'tech'.\\n\\nQuestion: {state['question']}\"\n",
        "    decision = llm.invoke(prompt).content\n",
        "    print(f\"PM Decision: Route to {decision}\")\n",
        "    if 'prd' in decision.lower():\n",
        "        return \"PRD_RESEARCHER\"\n",
        "    else:\n",
        "        return \"TECH_RESEARCHER\"\n",
        "\n",
        "# 3. Build the graph\n",
        "workflow_v3 = StateGraph(ResearchState)\n",
        "workflow_v3.add_node(\"PRD_RESEARCHER\", prd_researcher_agent)\n",
        "workflow_v3.add_node(\"TECH_RESEARCHER\", tech_researcher_agent)\n",
        "workflow_v3.add_node(\"SYNTHESIZE\", synthesizer_agent)\n",
        "workflow_v3.add_node(\"VALIDATE\", human_validation_node)\n",
        "\n",
        "workflow_v3.add_conditional_edges(\n",
        "    \"__start__\",\n",
        "    project_manager_router,\n",
        ")\n",
        "workflow_v3.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZE\")\n",
        "workflow_v3.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZE\")\n",
        "workflow_v3.add_edge(\"SYNTHESIZE\", \"VALIDATE\")\n",
        "workflow_v3.add_edge(\"VALIDATE\", END)\n",
        "\n",
        "app_v3 = workflow_v3.compile()\n",
        "\n",
        "print(\"\\n--- Invoking 5-Agent Research Team ---\")\n",
        "inputs = {\"question\": \"What framework was chosen and why?\"}\n",
        "# This will now require user input in the console to complete.\n",
        "# result = app_v3.invoke(inputs)\n",
        "# print(f\"Final Answer: {result['final_answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Conclusion\n",
        "\n",
        "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. Most importantly, you implemented a human-in-the-loop validation step, which is a critical pattern for building safe, reliable, and trustworthy AI applications. In the next lab, we will integrate this powerful system into our FastAPI backend."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}