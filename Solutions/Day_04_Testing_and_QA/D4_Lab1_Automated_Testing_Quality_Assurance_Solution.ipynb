{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 4 - Lab 1: Automated Testing & Quality Assurance (Solution)\n",
        "\n",
        "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
        "\n",
        "**Introduction:**\n",
        "This solution notebook provides the complete prompts and explanations for generating a robust test suite. It covers generating simple tests, refactoring them into more advanced patterns, and creating the necessary fixtures for professional-grade database testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "**Explanation:**\n",
        "We load the application's source code to provide the LLM with the necessary context to write accurate tests. A good prompt for test generation should always include the code that needs to be tested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project's root directory to the Python path\n",
        "try:\n",
        "    # This works when running as a script\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "except NameError:\n",
        "    # This works when running in an interactive environment (like a notebook)\n",
        "    # We go up two levels from the notebook's directory to the project root.\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import setup_llm_client, get_completion, save_artifact, load_artifact\n",
        "\n",
        "client, model_name, api_provider = setup_llm_client()\n",
        "\n",
        "# In a real scenario, you would load all relevant application files.\n",
        "# For this solution, we assume the context is available.\n",
        "try:\n",
        "    # Assuming the app is structured with main.py, crud.py, models.py, schemas.py\n",
        "    # A more robust script would concatenate these files.\n",
        "    main_code = load_artifact(\"app/main.py\") # Simplified for demonstration\n",
        "    full_app_context = f\"\"\"# main.py\\n{main_code}\\n\"\"\"\n",
        "    print(\"Successfully loaded application code context.\")\n",
        "except (FileNotFoundError, TypeError):\n",
        "    full_app_context = \"\"\n",
        "    print(\"Warning: Could not load application code. Lab may not function correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Challenges - Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
        "\n",
        "**Explanation:**\n",
        "This prompt asks for the most straightforward type of test: one that verifies the application works as expected when given valid input. We specifically ask for tests for the `POST` and `GET` endpoints. The prompt includes the full application code as context, which is crucial for the LLM to understand the API's structure, expected payloads, and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "happy_path_tests_prompt = f\"\"\"\n",
        "You are a Senior QA Engineer writing tests for a FastAPI application using pytest.\n",
        "\n",
        "Based on the application code provided below, please generate two 'happy path' test functions:\n",
        "1. A test named `test_create_user` for the `POST /users/` endpoint. It should create a user and assert that the status code is 200 and the response email matches the input.\n",
        "2. A test named `test_read_users` for the `GET /users/` endpoint. It should assert the status code is 200 and that the response is a list.\n",
        "\n",
        "**Application Code Context:**\n",
        "```python\n",
        "{full_app_context}\n",
        "```\n",
        "\n",
        "Your response should be only the raw Python code for the tests, including necessary imports like `TestClient`.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Happy Path Tests ---\")\n",
        "if full_app_context:\n",
        "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
        "    print(generated_happy_path_tests)\n",
        "else:\n",
        "    print(\"Skipping test generation because app context is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2 (Intermediate): Parameterizing Edge Case Tests\n",
        "\n",
        "**Explanation:**\n",
        "This is a two-step process that teaches a valuable refactoring pattern. First, we would generate individual tests for different error conditions. Then, we provide those generated tests back to the LLM and ask it to refactor them. The key instruction is to use `@pytest.mark.parametrize`, which allows a single test function to be run with multiple different inputs. This results in code that is much cleaner, more concise, and easier to maintain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This prompt assumes we have already generated two separate tests for invalid data.\n",
        "# For this solution, we combine the request into one.\n",
        "parametrize_refactor_prompt = f\"\"\"\n",
        "You are a Senior Python Developer who champions writing clean, maintainable test code.\n",
        "\n",
        "Based on the FastAPI application code below, I need a test for invalid user creation payloads. Instead of writing multiple separate tests, please generate a single, parameterized test function using `@pytest.mark.parametrize`.\n",
        "\n",
        "The test should be named `test_create_user_invalid_payload` and should check at least two scenarios:\n",
        "1. A payload with an invalid email address.\n",
        "2. A payload with a password that is too short (e.g., less than 6 characters).\n",
        "\n",
        "In both scenarios, the test must assert that the API returns a `422 Unprocessable Entity` status code.\n",
        "\n",
        "**Application Code Context:**\n",
        "```python\n",
        "{full_app_context}\n",
        "```\n",
        "\n",
        "Output only the raw Python code for the single parameterized test function.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Parameterized Edge Case Test ---\")\n",
        "if full_app_context:\n",
        "    generated_parameterized_test = get_completion(parametrize_refactor_prompt, client, model_name, api_provider)\n",
        "    print(generated_parameterized_test)\n",
        "else:\n",
        "    print(\"Skipping test generation because app context is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3 (Advanced): Testing with an Isolated Database\n",
        "\n",
        "**Explanation:**\n",
        "This is the most advanced and most important concept in this lab. Running tests against your real development database is risky and can lead to flaky tests. A professional testing setup uses an isolated, temporary database for each test run.\n",
        "\n",
        "This prompt asks the LLM to generate a `pytest` fixture. A fixture is a function that runs before each test. This particular fixture does the following:\n",
        "1.  Sets up an in-memory SQLite database.\n",
        "2.  Creates all the necessary tables.\n",
        "3.  Uses a FastAPI feature to override the `get_db` dependency, so that during the test, API endpoints connect to the temporary database instead of the real one.\n",
        "4.  Yields the database session to the test function.\n",
        "5.  After the test is finished, it cleans up and closes the connection.\n",
        "\n",
        "Generating this complex fixture saves a tremendous amount of time and boilerplate coding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_fixture_prompt = f\"\"\"\n",
        "You are an expert in Python testing with pytest and FastAPI.\n",
        "\n",
        "I need to create a `pytest` fixture to provide an isolated, in-memory SQLite database session for each test run. This is a critical best practice for testing database-connected applications.\n",
        "\n",
        "Please generate the Python code for a file named `tests/conftest.py` that contains this fixture.\n",
        "\n",
        "The fixture should:\n",
        "1. Be named `test_db`.\n",
        "2. Configure a SQLAlchemy engine for an in-memory SQLite database.\n",
        "3. Create all database tables before the test session starts.\n",
        "4. Override the `get_db` dependency from the main FastAPI app to yield a session from this test database.\n",
        "5. Clean up the database tables after the test session is complete.\n",
        "\n",
        "**Application Code Context:**\n",
        "```python\n",
        "{full_app_context}\n",
        "```\n",
        "\n",
        "Output only the raw Python code for the `conftest.py` file.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Pytest DB Fixture ---\")\n",
        "if full_app_context:\n",
        "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
        "    print(generated_db_fixture)\n",
        "    # In the lab, students would now create 'tests/conftest.py' and 'tests/test_main.py'\n",
        "    # and assemble all the generated test code into the final test suite.\n",
        "    # save_artifact(generated_db_fixture, \"tests/conftest.py\")\n",
        "else:\n",
        "    print(\"Skipping fixture generation because app context is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Conclusion\n",
        "\n",
        "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases, refactor tests for maintainability, and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality. In the next lab, we will use this test suite to help us debug and improve our code."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}