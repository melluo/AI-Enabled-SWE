{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A robust CI pipeline is the backbone of modern software development. It automatically builds and tests your code every time a change is made, catching bugs early and ensuring quality. In this lab, you will generate all the configuration-as-code artifacts needed to build a professional CI pipeline for our application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load our application code to provide context for the LLM. The AI needs to see our code's imports to generate an accurate `requirements.txt` file.\n",
    "\n",
    "**Model Selection:**\n",
    "Models that are good at understanding code and structured data formats like YAML are ideal. `gpt-4.1`, `o3`, or `codex-mini` are strong choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated configuration files.\n",
    "- `clean_llm_output()`: To clean up the generated text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4.1'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Task:** Before we can build a Docker image, we need a list of our Python dependencies. Prompt the LLM to analyze your application code and generate a `requirements.txt` file.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that provides the LLM with the source code of your FastAPI application (`app_code`).\n",
    "2.  Instruct it to analyze the `import` statements and generate a list of all external dependencies (like `fastapi`, `uvicorn`, `sqlalchemy`). You should also ask it to include `pytest` for testing.\n",
    "3.  The output should be formatted as a standard `requirements.txt` file.\n",
    "4.  Save the artifact to the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating requirements.txt ---\n",
      "# FastAPI for building the API\n",
      "fastapi==0.111.0     # The main web framework for building APIs\n",
      "\n",
      "# ASGI server for running FastAPI\n",
      "uvicorn[standard]==0.30.1    # ASGI server to run FastAPI apps in production/development\n",
      "\n",
      "# SQLAlchemy ORM for database models and sessions\n",
      "SQLAlchemy==2.0.30    # ORM library for database interactions\n",
      "\n",
      "# SQLite driver (already included with Python, but explicit for clarity)\n",
      "# No extra package needed for SQLite with SQLAlchemy\n",
      "\n",
      "# Pydantic for data validation and serialization\n",
      "pydantic==2.7.1       # Used by FastAPI for request/response validation\n",
      "\n",
      "# Pytest for running tests\n",
      "pytest==8.2.2         # Testing framework for writing and running tests\n",
      "\n",
      "# Optional: python-dotenv for loading environment variables (helpful in dev)\n",
      "python-dotenv==1.0.1  # Load environment variables from a .env file (optional, but recommended for config)\n",
      "\n",
      "# Optional: typing extensions for type hints (sometimes required by FastAPI/Pydantic)\n",
      "typing-extensions==4.11.0    # For improved type hinting support\n",
      "\n",
      "# Optional: HTTPX for async API testing (if planning to write API tests)\n",
      "httpx==0.27.0         # For testing FastAPI endpoints asynchronously\n",
      "```\n",
      "\n",
      "### Notes:\n",
      "\n",
      "- Comments are added after `#` for each package, which is supported by pip.\n",
      "- All versions are the latest stable as of June 2024.\n",
      "- If you don't use `python-dotenv` or `httpx`, you may remove those lines.\n",
      "- `sqlite3` is included in Python's standard library, so no extra package is needed for SQLite support with SQLAlchemy.\n",
      "\n",
      "---\n",
      "\n",
      "**Copy and save this file as `requirements.txt` in your project root.**  \n",
      "You can install all dependencies with:\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "✅ Successfully saved artifact to: requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate a requirements.txt file.\n",
    "requirements_prompt = f\"\"\"\n",
    "Using the provided application code {app_code}, generate a requirements.txt file that includes the following:\n",
    "1. All necessary Python packages required to run the application. (such as fastapi, uvicorn, sqlalchemy, pytest, etc.)\n",
    "2. Include specific versions of packages (use the latest stable versions).\n",
    "3. Include explanatory comments for each package, describing its purpose in the application.\n",
    "4. Ensure that the generated requirements.txt is formatted correctly for pip installation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Task:** Generate a multi-stage `Dockerfile` to create an optimized and secure container image for our application.\n",
    "\n",
    "> **Tip:** Why a multi-stage Dockerfile? The first stage (the 'builder') installs all dependencies, including build-time tools. The final stage copies only the application code and the necessary installed packages. This results in a much smaller, more secure production image because it doesn't contain any unnecessary build tools.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt asking for a multi-stage `Dockerfile` for a Python FastAPI application.\n",
    "2.  Specify the following requirements:\n",
    "    * Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "    * The first stage should install dependencies from `requirements.txt`.\n",
    "    * The final stage should copy the installed dependencies and the application code.\n",
    "    * The `CMD` should execute the application using `uvicorn`.\n",
    "3.  Save the generated file as `Dockerfile` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Dockerfile ---\n",
      "Dockerfile\n",
      "# ---- Builder Stage ----\n",
      "FROM python:3.11-slim AS builder\n",
      "\n",
      "# Set environment variables for Python\n",
      "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
      "    PYTHONUNBUFFERED=1\n",
      "\n",
      "# Create and set work directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Install build dependencies\n",
      "RUN apt-get update && \\\n",
      "    apt-get install --no-install-recommends -y build-essential gcc && \\\n",
      "    rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install pip-tools to handle requirements compilation (optional)\n",
      "RUN pip install --upgrade pip\n",
      "\n",
      "# Copy requirements to the container\n",
      "COPY requirements.txt .\n",
      "\n",
      "# Install Python dependencies into /install\n",
      "RUN pip install --prefix=/install -r requirements.txt\n",
      "\n",
      "# ---- Final Stage ----\n",
      "FROM python:3.11-slim\n",
      "\n",
      "# Set environment variables for Python\n",
      "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
      "    PYTHONUNBUFFERED=1\n",
      "\n",
      "# Create non-root user\n",
      "RUN adduser --disabled-password --no-create-home --gecos '' appuser\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy installed dependencies from builder\n",
      "COPY --from=builder /install /usr/local\n",
      "\n",
      "# Copy application code\n",
      "COPY . .\n",
      "\n",
      "# Change ownership to non-root user\n",
      "RUN chown -R appuser:appuser /app\n",
      "\n",
      "# Switch to non-root user\n",
      "USER appuser\n",
      "\n",
      "# Expose port for FastAPI\n",
      "EXPOSE 8000\n",
      "\n",
      "# Start FastAPI with uvicorn\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Notes:**\n",
      "- Replace `main:app` in the `CMD` with the actual module and FastAPI app object, if different.\n",
      "- This uses a non-root user for better security.\n",
      "- Only runtime essentials are included in the final image for a smaller attack surface.\n",
      "- `build-essential` and `gcc` are only in the builder stage.\n",
      "- Port 8000 is exposed, which is the FastAPI/uvicorn default.\n",
      "- Adjust the number of `--workers` as appropriate for your deployment.\n",
      "- If your `requirements.txt` installs only wheels (no compilation needed), you can further simplify by skipping build-essential and gcc.\n",
      "\n",
      "**Usage:**\n",
      "```bash\n",
      "docker build -t myfastapiapp .\n",
      "docker run -p 8000:8000 myfastapiapp\n",
      "✅ Successfully saved artifact to: Dockerfile\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate a multi-stage Dockerfile.\n",
    "dockerfile_prompt = \"\"\"\n",
    "Using the provided application code {app_code}, generate a multi-stage Dockerfile for a Python FastAPI application that includes the following:\n",
    "1. A slim Python base image.\n",
    "2. Install all necessary dependencies from requirements.txt.\n",
    "3. Copy installed dependencies and the application code into the container.\n",
    "4. Expose the application on port 8000.\n",
    "5. Use uvicorn to run the FastAPI application.\n",
    "6. Ensure that the Dockerfile is optimized for production use, including best practices for security and performance.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(cleaned_dockerfile)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Task:** Generate a complete GitHub Actions workflow file to automate the build and test process.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt to generate a GitHub Actions workflow file named `ci.yml`.\n",
    "2.  Specify the following requirements for the workflow:\n",
    "    * It should trigger on any `push` to the `main` branch.\n",
    "    * It should define a single job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "    * The job should have steps to: 1) Check out the code, 2) Set up a Python environment, 3) Install dependencies from `requirements.txt`, and 4) Run the test suite using `pytest`.\n",
    "3.  Save the generated YAML file to `.github/workflows/ci.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the GitHub Actions ci.yml file.\n",
    "ci_workflow_prompt = \"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    # Note: The save_artifact helper creates directories as needed.\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
