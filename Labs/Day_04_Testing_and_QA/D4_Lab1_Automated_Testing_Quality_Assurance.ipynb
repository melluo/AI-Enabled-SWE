{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
        "\n",
        "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
        "\n",
        "**Estimated Time:** 135 minutes\n",
        "\n",
        "**Introduction:**\n",
        "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "We will load the source code for the main application. For this lab, we assume the student has completed Day 3 and has a functional, multi-file FastAPI application in their `app/` directory. We will load the contents of `app/main.py` and other relevant files to provide the LLM with the full context of the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project's root directory to the Python path\n",
        "try:\n",
        "    # This works when running as a script\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "except NameError:\n",
        "    # This works when running in an interactive environment (like a notebook)\n",
        "    # We go up two levels from the notebook's directory to the project root.\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import setup_llm_client, get_completion, save_artifact, load_artifact\n",
        "\n",
        "client, model_name, api_provider = setup_llm_client()\n",
        "\n",
        "# Load the application code from Day 3 to provide context\n",
        "try:\n",
        "    # A more robust approach would be to load all .py files from the app directory\n",
        "    main_code = load_artifact(\"app/main.py\")\n",
        "    crud_code = load_artifact(\"app/crud.py\")\n",
        "    schemas_code = load_artifact(\"app/schemas.py\")\n",
        "    full_app_context = f\"\"\"# main.py\\n{main_code}\\n\\n# crud.py\\n{crud_code}\\n\\n# schemas.py\\n{schemas_code}\"\"\"\n",
        "    print(\"Successfully loaded application code context.\")\n",
        "except (FileNotFoundError, TypeError):\n",
        "    full_app_context = \"\"\n",
        "    print(\"Warning: Could not load all application code. Lab may not function correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
        "\n",
        "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
        "2.  Provide the `full_app_context`.\n",
        "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `200 OK` status code and verifying the response body).\n",
        "4.  Generate another test for the `GET /users/` endpoint.\n",
        "\n",
        "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Write a prompt to generate happy path tests for your API.\n",
        "happy_path_tests_prompt = f\"\"\"\n",
        "# Your prompt here\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Happy Path Tests ---\")\n",
        "if full_app_context:\n",
        "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
        "    print(generated_happy_path_tests)\n",
        "else:\n",
        "    print(\"Skipping test generation because app context is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2 (Intermediate): Parameterizing Edge Case Tests\n",
        "\n",
        "**Task:** Prompt the LLM to generate tests for common edge cases and then refactor them into a single, elegant, parameterized test function.\n",
        "\n",
        "**Instructions:**\n",
        "1.  First, prompt the LLM to write a test for creating a user with an invalid email, asserting that the API returns a `422 Unprocessable Entity` error.\n",
        "2.  Next, prompt the LLM to write a separate test for creating a user with a password that is too short (assuming your Pydantic model has a `min_length` constraint).\n",
        "3.  Finally, create a new prompt. Provide the two edge case tests you just generated and instruct the LLM to refactor them into a single test function that uses the `@pytest.mark.parametrize` decorator. This is a key pattern for writing clean and maintainable tests.\n",
        "\n",
        "**Expected Quality:** A single, parameterized `pytest` function that efficiently tests multiple invalid input scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Prompt for the individual edge case tests first.\n",
        "invalid_email_test_prompt = \"\"\" # Your prompt for invalid email test \"\"\"\n",
        "short_password_test_prompt = \"\"\" # Your prompt for short password test \"\"\"\n",
        "# generated_invalid_email_test = get_completion(invalid_email_test_prompt, ...)\n",
        "# generated_short_password_test = get_completion(short_password_test_prompt, ...)\n",
        "\n",
        "# TODO: Write a prompt to refactor the edge case tests using pytest.mark.parametrize.\n",
        "parametrize_refactor_prompt = f\"\"\"\n",
        "# Your prompt here. Provide the two separate tests as context to be refactored.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Parameterized Edge Case Test ---\")\n",
        "if full_app_context:\n",
        "    generated_parameterized_test = get_completion(parametrize_refactor_prompt, client, model_name, api_provider)\n",
        "    print(generated_parameterized_test)\n",
        "else:\n",
        "    print(\"Skipping test generation because app context is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3 (Advanced): Testing with an Isolated Database\n",
        "\n",
        "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Create a prompt that asks the LLM to generate a `pytest` fixture named `test_db`.\n",
        "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
        "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
        "4.  It also needs to override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
        "5.  Once you have the fixture, manually create a `tests/test_main.py` file and add the fixture to it. Then, refactor the happy path tests from Challenge 1 to accept `test_db` as an argument, ensuring they run against the isolated database instead of the real one.\n",
        "\n",
        "**Expected Quality:** A `tests/test_main.py` file containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
        "db_fixture_prompt = f\"\"\"\n",
        "# Your prompt here\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Pytest DB Fixture ---\")\n",
        "if full_app_context:\n",
        "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
        "    print(generated_db_fixture)\n",
        "    # TODO: Manually create 'tests/test_main.py' and add this fixture.\n",
        "    # Then, refactor your other tests to use it.\n",
        "else:\n",
        "    print(\"Skipping fixture generation because app context is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Conclusion\n",
        "\n",
        "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases, refactor tests for maintainability, and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality. In the next lab, we will use this test suite to help us debug and improve our code."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}