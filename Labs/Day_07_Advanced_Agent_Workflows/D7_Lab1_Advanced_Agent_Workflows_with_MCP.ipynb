{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7 - Lab 1: Advanced Agent Workflows with MCP\n",
    "\n",
    "**Objective:** Learn to structure complex prompts for AI agents using the Model Context Protocol (MCP) to improve reliability and clarity, and build a code refactoring agent that leverages this protocol.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 7! As agentic systems become more complex, sending a simple, unstructured prompt is often not enough. The Model Context Protocol (MCP) provides a standardized, XML-like way to structure the information you send to an agent, clearly separating instructions, user requests, and different types of context. In this lab, you will use the MCP SDK and LangChain adapters to build a more robust and predictable code refactoring agent.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will install the necessary libraries for this lab, including `model-context-protocol` for creating structured prompts and `langchain-mcp-adapters` for easily integrating MCP with our LangChain agents.\n",
    "\n",
    "**Model Selection:**\n",
    "For tasks involving structured data and code, models with strong reasoning and instruction-following capabilities are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_context_protocol not found, installing...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['c:\\\\Users\\\\labadmin\\\\.conda\\\\envs\\\\venv\\\\python.exe', '-m', 'pip', 'install', '-q', 'model_context_protocol']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36minstall_if_missing\u001b[39m\u001b[34m(package)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\labadmin\\.conda\\envs\\venv\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'model_context_protocol'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m     20\u001b[39m         subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-q\u001b[39m\u001b[33m\"\u001b[39m, package])\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43minstall_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_context_protocol\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m install_if_missing(\u001b[33m'\u001b[39m\u001b[33mlangchain_mcp_adapters\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_llm_client, get_completion\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36minstall_if_missing\u001b[39m\u001b[34m(package)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found, installing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-q\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\labadmin\\.conda\\envs\\venv\\Lib\\subprocess.py:413\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m         cmd = popenargs[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['c:\\\\Users\\\\labadmin\\\\.conda\\\\envs\\\\venv\\\\python.exe', '-m', 'pip', 'install', '-q', 'model_context_protocol']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('model_context_protocol')\n",
    "install_if_missing('langchain_mcp_adapters')\n",
    "\n",
    "from utils import setup_llm_client, get_completion\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Manually Formatting an MCP Prompt\n",
    "\n",
    "**Task:** Before using the SDK, manually write a prompt string that follows the MCP XML-style format.\n",
    "\n",
    "> **Tip:** Notice the XML-style tags. This structure isn't just for looks; it gives the LLM clear signals about the role of each piece of information. `<request>` is the user's goal, while `<context>` provides the data needed to achieve it.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a multi-line f-string for your prompt.\n",
    "2.  The prompt should include the following MCP tags:\n",
    "    * `<request>`: The user's high-level request (e.g., \"Please refactor this code.\").\n",
    "    * `<context>`: A container for contextual information.\n",
    "    * `<code>`: Inside `<context>`, place a snippet of Python code that needs refactoring.\n",
    "    * `<instructions>`: Inside `<context>`, provide specific instructions for the refactoring (e.g., \"Make this function more readable and add type hints.\").\n",
    "3.  Send this manually formatted string to the LLM and observe the output.\n",
    "\n",
    "**Expected Quality:** A successful response from the LLM, demonstrating that it can understand and act upon the structured MCP format even without the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sending Manually Formatted MCP Prompt ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      4\u001b[39m manual_mcp_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m<request>\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mPlease refactor this code.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33m</request>\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Sending Manually Formatted MCP Prompt ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m response = \u001b[43mget_completion\u001b[49m(manual_mcp_prompt, client, model_name, api_provider)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_completion' is not defined"
     ]
    }
   ],
   "source": [
    "code_to_refactor = \"def my_func(a,b):\\n  x=a+b\\n  y=x*2\\n  return y\"\n",
    "\n",
    "# TODO: Create a prompt string that manually uses MCP tags.\n",
    "manual_mcp_prompt = f\"\"\"\n",
    "<request>\n",
    "Please refactor this code.\n",
    "<context>\n",
    "    <code>{code_to_refactor}</code>\n",
    "    <instructions>\n",
    "Make this function more readable and add type hints.\n",
    "    </instructions>\n",
    "</context>\n",
    "</request>\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Sending Manually Formatted MCP Prompt ---\")\n",
    "response = get_completion(manual_mcp_prompt, client, model_name, api_provider)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Using the MCP SDK to Build a Prompt\n",
    "\n",
    "**Task:** Now, use the `model-context-protocol` SDK to programmatically build the same structured prompt.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Import `Request`, `Context`, `Code`, and `Instructions` from the `model_context_protocol` library.\n",
    "2.  Create an instance of the `Code` context item, passing the `code_to_refactor`.\n",
    "3.  Create an instance of the `Instructions` context item.\n",
    "4.  Create a `Context` object, passing a list containing your `Code` and `Instructions` items.\n",
    "5.  Create the final `Request` object, passing the user's request text and the `Context` object.\n",
    "6.  Use the `.render()` method on the `Request` object to get the formatted string and print it to verify it matches your manual prompt.\n",
    "\n",
    "**Expected Quality:** A Python script that programmatically generates a valid MCP-formatted string, demonstrating a more robust and maintainable way to construct complex prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_context_protocol import Request, Context, Code, Instructions\n",
    "\n",
    "# TODO: Use the MCP SDK to build the request programmatically.\n",
    "\n",
    "# 1. Create Code and Instructions items\n",
    "code_item = None # Your code here\n",
    "instructions_item = None # Your code here\n",
    "\n",
    "# 2. Create the Context object\n",
    "context_obj = None # Your code here\n",
    "\n",
    "# 3. Create the final Request object\n",
    "mcp_request = None # Your code here\n",
    "\n",
    "# 4. Render the request to a string\n",
    "rendered_prompt = mcp_request.render()\n",
    "\n",
    "print(\"--- Programmatically Rendered MCP Prompt ---\")\n",
    "print(rendered_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Building a Context-Aware Refactoring Agent\n",
    "\n",
    "**Task:** Use the `langchain-mcp-adapters` library to create a LangChain agent that automatically structures its inputs using MCP.\n",
    "\n",
    "> **What do the adapters do?** The `langchain-mcp-adapters` library acts as a bridge. It lets you define the high-level structure of your context (e.g., 'I need code and instructions') and automatically handles the work of formatting it into the proper MCP string for the LLM.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Import `McpChatPromptTemplate` from `langchain_mcp_adapters`.\n",
    "2.  Create a list of context builders. For this lab, you'll need one for `Code` and one for `Instructions`. The adapter library provides these.\n",
    "3.  Create an instance of `McpChatPromptTemplate`, passing in your list of context builders and a request template string.\n",
    "4.  Create a simple LangChain chain by piping this special prompt template to your LLM.\n",
    "5.  Invoke the chain. The input should be a dictionary containing keys that match your context builders (e.g., `code` and `instructions`) and your request template variables.\n",
    "6.  The adapter will automatically build the full MCP-formatted prompt before sending it to the LLM.\n",
    "\n",
    "**Expected Quality:** A functioning LangChain agent that seamlessly and automatically uses the Model Context Protocol, demonstrating how to build robust, production-ready agents that handle complex, multi-part context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters import McpChatPromptTemplate\n",
    "from langchain_mcp_adapters.context_builders import CodeContextBuilder, InstructionsContextBuilder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "\n",
    "# TODO: 1. Create a list of context builders\n",
    "context_builders = [] # Your list here\n",
    "\n",
    "# TODO: 2. Create the McpChatPromptTemplate\n",
    "mcp_prompt_template = None # Your prompt template here\n",
    "\n",
    "# TODO: 3. Create the LangChain agent chain\n",
    "refactoring_agent = None # Your chain here\n",
    "\n",
    "# TODO: 4. Invoke the agent with a dictionary of inputs\n",
    "refactoring_input = {\n",
    "    \"user_request\": \"Please refactor this code to be more Pythonic.\",\n",
    "    \"code\": \"def f(data):\\n  r = []\\n  for i in data:\\n    if i % 2 == 0:\\n      r.append(i*i)\\n  return r\",\n",
    "    \"instructions\": \"Use a list comprehension and add type hints.\"\n",
    "}\n",
    "\n",
    "print(\"--- Invoking MCP-powered Refactoring Agent ---\")\n",
    "refactored_code = refactoring_agent.invoke(refactoring_input)\n",
    "print(refactored_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have learned how to use the Model Context Protocol to create structured, reliable prompts for your AI agents. You progressed from manually writing MCP-formatted strings to using the SDK for programmatic construction, and finally to using the LangChain adapter for seamless integration. This skill is crucial for building advanced agents that need to handle diverse and complex contextual information in a predictable way.\n",
    "\n",
    "> **Key Takeaway:** Structuring prompts with a clear protocol like MCP makes agents more reliable. It separates the user's *request* from the *context* the agent needs, reducing ambiguity and allowing you to build more predictable, production-ready AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
