{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 8 - Lab 1: Vision-Enabled UI/UX Agents\n",
        "\n",
        "**Objective:** Use multi-modal vision models to generate a frontend UI from a design image, and then use a second agent to perform an automated design review.\n",
        "\n",
        "**Estimated Time:** 135 minutes\n",
        "\n",
        "**Introduction:**\n",
        "Welcome to the final day of core instruction! Today, we'll explore one of the most exciting advancements in AI: vision. We will use a vision-capable model to act as a frontend developer, translating a design image directly into code. Then, we will create a second \"UI/UX Critic\" agent to automate the design review process, demonstrating a complete, AI-assisted frontend workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "For this lab, we need to ensure we are using a vision-capable model. We will configure our `utils.py` helper to use a model like OpenAI's `gpt-4o`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project's root directory to the Python path\n",
        "try:\n",
        "    # This works when running as a script\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "except NameError:\n",
        "    # This works when running in an interactive environment (like a notebook)\n",
        "    # We go up two levels from the notebook's directory to the project root.\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import setup_llm_client, get_vision_completion, save_artifact\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Ensure you select a vision-capable model\n",
        "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
        "\n",
        "if not model_name:\n",
        "    print(\"Could not set up a valid LLM client. Please check your .env file and utils.py configuration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Design Screenshot\n",
        "\n",
        "This is the design we want our AI agent to build. It's a simple login form component. We will provide the URL to this image directly to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "login_form_url = \"https://i.imgur.com/s42SYz6.png\"\n",
        "display(Image(url=login_form_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: The Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1 (Foundational): Generating a Monolithic UI Component\n",
        "\n",
        "**Task:** Use the vision model to generate a single, self-contained React component that replicates the design from the image.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Create a prompt that asks the vision model to act as an expert frontend developer.\n",
        "2.  The prompt should instruct the model to analyze the image at the provided URL.\n",
        "3.  Ask it to generate a single React component using Tailwind CSS for styling.\n",
        "4.  The output should be a single block of JSX code.\n",
        "\n",
        "**Expected Quality:** A single file's worth of React code that, when rendered, visually approximates the login form in the screenshot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Write a prompt to generate a single React component from the image.\n",
        "generate_ui_prompt = f\"\"\"\n",
        "# Your prompt here\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Generating Monolithic UI Component ---\")\n",
        "if model_name:\n",
        "    generated_monolithic_code = get_vision_completion(generate_ui_prompt, login_form_url, client, model_name, api_provider)\n",
        "    print(generated_monolithic_code)\n",
        "else:\n",
        "    print(\"Skipping UI generation because no valid model is configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2 (Intermediate): Refactoring into Reusable Components\n",
        "\n",
        "**Task:** A single, large component is not good practice. Now, prompt the LLM to refactor the monolithic code it just generated into smaller, reusable sub-components.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Create a new prompt.\n",
        "2.  Provide the monolithic JSX code from the previous step as context.\n",
        "3.  Instruct the LLM to act as a senior frontend developer who champions clean code.\n",
        "4.  Ask it to refactor the code by creating smaller, reusable components (e.g., `<StyledButton>`, `<InputWithIcon>`).\n",
        "5.  The final output should be the complete code with the new, smaller components defined and used within the main `Login` component.\n",
        "\n",
        "**Expected Quality:** A well-structured React file that demonstrates the component-based architecture, which is a fundamental best practice in modern frontend development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Write a prompt to refactor the monolithic code into smaller components.\n",
        "refactor_ui_prompt = f\"\"\"\n",
        "# Your prompt here. Remember to provide the code generated in the previous step as context.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Refactoring UI into Components ---\")\n",
        "if 'generated_monolithic_code' in locals():\n",
        "    refactored_code = get_completion(refactor_ui_prompt, client, model_name, api_provider)\n",
        "    print(refactored_code)\n",
        "else:\n",
        "    print(\"Skipping refactoring because monolithic code was not generated.\")\n",
        "    refactored_code = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3 (Advanced): The AI UI/UX Critic Agent\n",
        "\n",
        "**Task:** Create a new \"UI/UX Critic\" agent. This agent will be given both the original design image and the generated code, and its job is to perform an automated design review.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Create a final, complex prompt for a new agent.\n",
        "2.  The prompt should instruct the agent to act as a meticulous UI/UX designer.\n",
        "3.  Provide the agent with two pieces of context: the URL of the original design image and the final, refactored React code.\n",
        "4.  The agent's task is to compare the code's likely rendered output to the design image and list any visual inconsistencies in spacing, font size, color, or layout.\n",
        "\n",
        "**Expected Quality:** A critical design review in markdown format. This demonstrates a powerful AI-on-AI workflow, where one AI generates work and another AI validates it, automating a time-consuming QA step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Write a prompt for the UI/UX Critic agent.\n",
        "critic_agent_prompt = f\"\"\"\n",
        "# Your prompt here. Provide both the image URL and the refactored code as context.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Invoking UI/UX Critic Agent ---\")\n",
        "if refactored_code:\n",
        "    design_review = get_vision_completion(critic_agent_prompt, login_form_url, client, model_name, api_provider)\n",
        "    print(design_review)\n",
        "    save_artifact(design_review, \"artifacts/design_review.md\")\n",
        "else:\n",
        "    print(\"Skipping critic agent because refactored code is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Conclusion\n",
        "\n",
        "Fantastic! You have completed a full, end-to-end frontend development workflow using multiple AI agents. You used a vision-powered agent to generate code from a design, a refactoring agent to improve the code's structure, and a critic agent to perform an automated design review. This powerful combination of skills can dramatically accelerate the process of turning visual ideas into functional user interfaces."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}