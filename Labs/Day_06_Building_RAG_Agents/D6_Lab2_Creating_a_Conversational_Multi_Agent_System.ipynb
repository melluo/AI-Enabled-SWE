{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 6 - Lab 2: Creating a Conversational Multi-Agent System\n",
        "\n",
        "**Objective:** Integrate the multi-agent LangGraph system from the previous lab into the FastAPI backend, creating a new `/chat` endpoint that is stateful and can handle conversational memory.\n",
        "\n",
        "**Estimated Time:** 135 minutes\n",
        "\n",
        "**Introduction:**\n",
        "A powerful agent isn't very useful if users can't interact with it. In this lab, you will take the sophisticated multi-agent research team you built in the previous lab and expose it as a conversational API endpoint. This involves integrating the LangGraph application into your FastAPI backend and, most importantly, implementing a mechanism to handle conversational memory, allowing for natural, multi-turn interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n",
        "\n",
        "For this lab, you will be working directly in your `app/` directory, primarily modifying your `main.py` file. The goal is to add a new endpoint that can run your LangGraph agent. We will mock the agent object here for development purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project's root directory to the Python path\n",
        "try:\n",
        "    # This works when running as a script\n",
        "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "except NameError:\n",
        "    # This works when running in an interactive environment (like a notebook)\n",
        "    # We go up two levels from the notebook's directory to the project root.\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uuid\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# This is a mock of the compiled LangGraph app from the previous lab.\n",
        "# In a real application, you would import and instantiate your actual graph here.\n",
        "class MockLangGraphApp:\n",
        "    def invoke(self, inputs, config):\n",
        "        print(f\"Invoking agent for session {config.get('configurable', {}).get('session_id')}\")\n",
        "        question = inputs.get('question')\n",
        "        return {\"final_answer\": f\"This is a mock answer to the question: '{question}'\"}\n",
        "\n",
        "multi_agent_app = MockLangGraphApp()\n",
        "\n",
        "# In-memory store for conversation histories\n",
        "conversation_histories = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: The Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 1 (Foundational): Creating a Stateless Chat Endpoint\n",
        "\n",
        "**Task:** Create a simple `/chat` endpoint that takes a user's question and returns the agent's response, without any memory.\n",
        "\n",
        "**Instructions:**\n",
        "1.  In your `app/main.py` file, define a new Pydantic model `ChatRequest` that has a single field: `question: str`.\n",
        "2.  Create a new `POST /chat` endpoint.\n",
        "3.  This endpoint should take a `ChatRequest` object as its body.\n",
        "4.  Inside the endpoint, call the `multi_agent_app.invoke()` method with the user's question.\n",
        "5.  Return the `final_answer` from the agent's response.\n",
        "\n",
        "**Expected Quality:** A functional, though stateless, API endpoint that allows a user to ask a single question and get a single answer from the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: This code should be added to your app/main.py file\n",
        "\n",
        "# 1. Define the Pydantic model for the request\n",
        "class ChatRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "# 2. Create the stateless POST /chat endpoint\n",
        "# @app.post(\"/chat\")\n",
        "# def chat_endpoint(request: ChatRequest):\n",
        "#     # Your implementation here\n",
        "#     # Tip: You'll need to invoke the multi_agent_app and return the answer.\n",
        "#     pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2 (Intermediate): Building a Simple Streamlit UI\n",
        "\n",
        "**Task:** Create a simple user interface using Streamlit to interact with your new `/chat` endpoint.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Create a new Python script named `chat_ui.py`.\n",
        "2.  Use Streamlit components to create a title, a text input box for the user's question, and a button to submit.\n",
        "3.  When the button is clicked, use the `requests` library to make a `POST` call to your (locally running) FastAPI server's `/chat` endpoint.\n",
        "4.  Display the agent's response on the page.\n",
        "\n",
        "**Expected Quality:** A functional web interface where a user can type a question, click a button, and see the agent's response, demonstrating a full-stack interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a file named 'chat_ui.py' and add the Streamlit code.\n",
        "# You will need to import streamlit and requests.\n",
        "\n",
        "# st.title(...)\n",
        "# question = st.text_input(...)\n",
        "# if st.button(...):\n",
        "#     response = requests.post(...)\n",
        "#     st.write(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3 (Advanced): Implementing Conversational Memory\n",
        "\n",
        "**Task:** Refactor your backend to handle stateful, multi-turn conversations. The agent should be able to remember the context of previous messages in the same session.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Modify your `ChatRequest` Pydantic model in `app/main.py` to include an optional `session_id: str = None`.\n",
        "2.  Create a new `POST /stateful_chat` endpoint logic:\n",
        "    * If the incoming request has no `session_id`, generate a new one using `uuid.uuid4()`.\n",
        "    * When calling `multi_agent_app.invoke()`, pass a `config` dictionary. This config must contain a `configurable` key with a `session_id` inside it. This is how LangGraph manages stateful conversations.\n",
        "    * Return both the agent's answer and the `session_id` to the client.\n",
        "3.  Update your `chat_ui.py` to store the `session_id` in `st.session_state` and send it back with each subsequent request.\n",
        "\n",
        "**Expected Quality:** A fully conversational chatbot. You should be able to ask a follow-up question (e.g., \"Can you tell me more about that?\") and have the agent understand the context from the previous turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: This code should replace the stateless endpoint in your app/main.py file.\n",
        "\n",
        "# 1. Define the new Pydantic model\n",
        "class StatefulChatRequest(BaseModel):\n",
        "    question: str\n",
        "    session_id: str | None = None\n",
        "\n",
        "# 2. Create the new stateful endpoint\n",
        "# @app.post(\"/stateful_chat\")\n",
        "# def stateful_chat_endpoint(request: StatefulChatRequest):\n",
        "#     # Tip: Get or create a session_id.\n",
        "\n",
        "#     # Tip: Prepare the inputs and config for LangGraph.\n",
        "\n",
        "#     # Tip: Invoke the agent with the config.\n",
        "\n",
        "#     # Tip: Return the answer and session_id.\n",
        "#     pass\n",
        "\n",
        "# TODO: Update your chat_ui.py to handle the session_id.\n",
        "# Tip: Store the session_id in st.session_state after the first response.\n",
        "# Tip: Include the session_id in the JSON payload for subsequent requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Conclusion\n",
        "\n",
        "Excellent! You have successfully integrated your powerful multi-agent system into a real-world application. You created a stateless API endpoint, built a UI for it, and then performed the critical upgrade to make it a stateful, conversational agent with memory. This is the complete pattern for deploying sophisticated AI assistants that can engage in natural, multi-turn dialogues with users."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}