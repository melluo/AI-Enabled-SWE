{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Estimated Time:** 180 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 6! Today, we build one of the most powerful and common patterns for enterprise AI: a system that can answer questions about your private documents. We will use LangGraph to create a 'research team' of AI agents. Each agent will have a specific job, and LangGraph will act as the manager, orchestrating their collaboration to find the best possible answer.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We need several libraries for this lab. `langgraph` is the core orchestrator, `langchain` provides the building blocks, `faiss-cpu` is for our vector store, and `pypdf` is for loading documents.\n",
    "\n",
    "**Model Selection:**\n",
    "For RAG and agentic workflows, models with strong instruction-following and reasoning are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `load_artifact()`: To read the project documents that will form our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n",
      "âœ… LLM Client configured: Using 'openai' with model 'gpt-4.1'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "An agent is only as smart as the information it can access. We will create a vector store containing all the project artifacts we've created so far. This will be our agent's 'knowledge base'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 13 document splits...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/day1_prd.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Task:** Build a simple LangGraph with two nodes: one to retrieve documents and one to generate an answer.\n",
    "\n",
    "> **Tip:** Think of `AgentState` as the shared 'whiteboard' for your agent team. Every agent (or 'node' in the graph) can read from and write to this state, allowing them to pass information to each other as they work on a problem.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Define the state for your graph using a `TypedDict`. It should contain keys for `question` and `documents`.\n",
    "2.  Create a \"Retriever\" node. This is a Python function that takes the state, uses the `retriever` to get relevant documents, and updates the state with the results.\n",
    "3.  Create a \"Generator\" node. This function takes the state, creates a prompt with the question and retrieved documents, calls the LLM, and stores the answer.\n",
    "4.  Build the `StateGraph`, add the nodes, and define the edges (`RETRIEVE` -> `GENERATE`).\n",
    "5.  Compile the graph and invoke it with a question about your project.\n",
    "\n",
    "**Expected Quality:** A functional graph that can answer a simple question (e.g., \"What is the purpose of this project?\") by retrieving context from the project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the state graph...\n",
      "adding nodes to the graph...\n",
      "adding edges to the graph...\n",
      "compiling the graph...\n",
      "Retrieving documents...\n",
      "Generating answer...\n",
      "Generated answer: The purpose of this project is to develop a new hire onboarding tool that includes an AI-powered semantic search feature to help personalize learning paths for new employees. The tool aims to make the\n",
      "onboarding process more effective by leveraging semantic search capabilities, allowing new hires to easily find relevant information and resources. The project specifically chooses to use PostgreSQL\n",
      "with the pgvector extension for semantic search, prioritizing reduced learning curve, operational simplicity, and cost-effectiveness by building on familiar and existing database infrastructure. The\n",
      "ultimate goal is to create an onboarding experience that is efficient, scalable, and tailored to individual needs through AI-driven personalization.\n",
      "Documents used for answer generation:\n",
      "Document 1: - **Future Work:**   - Monitor the system's performance and scalability as the dataset grows to determine if a transition to a specialized vector database becomes necessary.   - Evaluate the need for\n",
      "potential architectural adjustments to accommodate high-dimensional vector data and improve search performance if required. ```\n",
      "Document 2: FutureWork(description=\"AI-powered personalized learning paths for new hires.\")     ],     appendix=[         AppendixItem(type=\"Open Question\", description=\"Which team will be responsible for\n",
      "maintaining the content in the document repository?\"),         AppendixItem(type=\"Dependency\", description=\"The final UI design mockups are required from the Design team by [Date].\")     ] )\n",
      "Document 3: ## Context - The project involves developing a new hire onboarding tool that requires a semantic search feature. The decision to use PostgreSQL with the `pgvector` extension over specialized vector\n",
      "databases such as ChromaDB, FAISS, or Weaviate involves several considerations. - PostgreSQL is a well-established relational database system, familiar to many development teams, which can reduce the\n",
      "initial learning curve and integration time. - Leveraging PostgreSQL with `pgvector` allows the use of existing database infrastructure, minimizing operational complexity by avoiding the need to\n",
      "manage a separate system for vector storage and search. - Cost-effectiveness is another important factor, as integrating `pgvector` into a current PostgreSQL setup may incur fewer additional costs\n",
      "than adopting a new technology stack.\n",
      "Document 4: ## Decision - The project will adopt PostgreSQL with the `pgvector` extension to implement the semantic search feature for the new hire onboarding tool.  ## Consequences - **Positive Outcomes:**   -\n",
      "Reduced learning curve and integration time due to team familiarity with PostgreSQL.   - Simplified operations by utilizing the existing database infrastructure.   - Cost savings from avoiding the\n",
      "introduction of a new technology stack.   - Ability to combine vector operations with traditional relational data queries.   - Maintained transactional integrity with PostgreSQL's ACID compliance.  -\n",
      "**Negative Trade-offs:**   - Potential performance and scalability limitations, especially with very large datasets or high throughput requirements.   - Possible lack of advanced features found in\n",
      "specialized vector databases, such as certain indexing techniques and optimized search algorithms for high-dimensional data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the purpose of this project',\n",
       " 'documents': [Document(id='b31691e6-9045-4580-9bf9-ec4dab2691ae', metadata={'source': 'artifacts/adr_001_database_choice.md'}, page_content=\"- **Future Work:**\\n  - Monitor the system's performance and scalability as the dataset grows to determine if a transition to a specialized vector database becomes necessary.\\n  - Evaluate the need for potential architectural adjustments to accommodate high-dimensional vector data and improve search performance if required.\\n```\"),\n",
       "  Document(id='2c0616e0-5177-4ebb-bd96-a91e90ccc75e', metadata={'source': 'artifacts/day1_prd.md'}, page_content='FutureWork(description=\"AI-powered personalized learning paths for new hires.\")\\n    ],\\n    appendix=[\\n        AppendixItem(type=\"Open Question\", description=\"Which team will be responsible for maintaining the content in the document repository?\"),\\n        AppendixItem(type=\"Dependency\", description=\"The final UI design mockups are required from the Design team by [Date].\")\\n    ]\\n)'),\n",
       "  Document(id='d107dfb5-c58f-4f5a-a596-35db881d8cad', metadata={'source': 'artifacts/adr_001_database_choice.md'}, page_content='## Context\\n- The project involves developing a new hire onboarding tool that requires a semantic search feature. The decision to use PostgreSQL with the `pgvector` extension over specialized vector databases such as ChromaDB, FAISS, or Weaviate involves several considerations.\\n- PostgreSQL is a well-established relational database system, familiar to many development teams, which can reduce the initial learning curve and integration time.\\n- Leveraging PostgreSQL with `pgvector` allows the use of existing database infrastructure, minimizing operational complexity by avoiding the need to manage a separate system for vector storage and search.\\n- Cost-effectiveness is another important factor, as integrating `pgvector` into a current PostgreSQL setup may incur fewer additional costs than adopting a new technology stack.'),\n",
       "  Document(id='435d95b3-2fe5-4835-aac7-7ad58bb33abd', metadata={'source': 'artifacts/adr_001_database_choice.md'}, page_content=\"## Decision\\n- The project will adopt PostgreSQL with the `pgvector` extension to implement the semantic search feature for the new hire onboarding tool.\\n\\n## Consequences\\n- **Positive Outcomes:**\\n  - Reduced learning curve and integration time due to team familiarity with PostgreSQL.\\n  - Simplified operations by utilizing the existing database infrastructure.\\n  - Cost savings from avoiding the introduction of a new technology stack.\\n  - Ability to combine vector operations with traditional relational data queries.\\n  - Maintained transactional integrity with PostgreSQL's ACID compliance.\\n\\n- **Negative Trade-offs:**\\n  - Potential performance and scalability limitations, especially with very large datasets or high throughput requirements.\\n  - Possible lack of advanced features found in specialized vector databases, such as certain indexing techniques and optimized search algorithms for high-dimensional data.\")],\n",
       " 'answer': 'The purpose of this project is to develop a new hire onboarding tool that includes an AI-powered semantic search feature to help personalize learning paths for new employees. The tool aims to make the onboarding process more effective by leveraging semantic search capabilities, allowing new hires to easily find relevant information and resources. The project specifically chooses to use PostgreSQL with the pgvector extension for semantic search, prioritizing reduced learning curve, operational simplicity, and cost-effectiveness by building on familiar and existing database infrastructure. The ultimate goal is to create an onboarding experience that is efficient, scalable, and tailored to individual needs through AI-driven personalization.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Write the code for the single-agent RAG system using LangGraph.\n",
    "# This will involve defining the state, the nodes, and the graph itself.\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "import textwrap\n",
    "class State(TypedDict):\n",
    "    \"\"\"State for the RAG system.\"\"\"\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retriever_node(state):\n",
    "    \"\"\"\n",
    "    Retriever node for LangGraph RAG system.\n",
    "    Takes the state dict, uses the retriever to get relevant documents,\n",
    "    and updates the state with the retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving documents...\")\n",
    "    question = state[\"question\"]\n",
    "    # Use the retriever to get relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    # Update the state with the retrieved documents\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def generator_node(state):\n",
    "    \"\"\" Generator node for LangGraph RAG system.\n",
    "    Takes the state dict, uses the LLM to generate an answer based on the question and documents,\n",
    "    and updates the state with the generated answer.\n",
    "    \"\"\"\n",
    "    print(\"Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents =state[\"documents\"] \n",
    "    prompt = f\"\"\"You are an AI assistant that answers questions based on the provided documents.\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    if answer is not None:\n",
    "        wrapped = textwrap.fill(str(answer), width=200)\n",
    "        print(f\"Generated answer: {wrapped}\")\n",
    "    else:\n",
    "        print(\"No answer generated.\")\n",
    "\n",
    "    print (\"Documents used for answer generation:\")\n",
    "    for idx, doc in enumerate(documents, start=1):\n",
    "        wrapped_doc = textwrap.fill(str(doc.page_content), width=200)\n",
    "        print(f\"Document {idx}: {wrapped_doc}\")\n",
    "    # Typed dictionary stuff\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Define the state graph for the RAG system\n",
    "print(\"Creating the state graph...\")\n",
    "graph = StateGraph(State)\n",
    "print(\"adding nodes to the graph...\")\n",
    "graph.add_node(\"RETRIEVE\", retriever_node)\n",
    "graph.add_node(\"GENERATE\", generator_node)\n",
    "\n",
    "print(\"adding edges to the graph...\")\n",
    "graph.set_entry_point(\"RETRIEVE\")\n",
    "graph.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
    "\n",
    "print(\"compiling the graph...\")\n",
    "app = graph.compile()\n",
    "\n",
    "app.invoke(State(question=\"What is the purpose of this project\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Task:** Add a second agent to your graph that acts as a \"Grader,\" deciding if the retrieved documents are relevant enough to answer the question.\n",
    "\n",
    "> **What is a conditional edge?** It's a decision point. After a node completes its task (like our 'Grader'), the conditional edge runs a function to decide which node to go to next. This allows your agent to change its plan based on new information.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Keep your `RETRIEVE` and `GENERATE` nodes from the previous challenge.\n",
    "2.  Create a new \"Grader\" node. This function takes the state (question and documents) and calls an LLM with a specific prompt: \"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\"\n",
    "3.  Add a **conditional edge** to your graph. After the `RETRIEVE` node, the graph should go to the `GRADE` node. After the `GRADE` node, it should check the grader's response. If 'yes', it proceeds to the `GENERATE` node. If 'no', it goes to an `END` node, concluding that it cannot answer the question.\n",
    "\n",
    "**Expected Quality:** A more robust graph that can gracefully handle cases where its knowledge base doesn't contain the answer, preventing it from hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the state graph...\n",
      "adding nodes to the graph...\n",
      "adding edges to the graph...\n",
      "Retrieving documents...\n",
      "Grading the answer...\n",
      "Continuing to generate answer...\n",
      "Documents are sufficient, proceeding to answer generation.\n",
      "Generating answer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What project doing?',\n",
       " 'documents': [Document(id='d107dfb5-c58f-4f5a-a596-35db881d8cad', metadata={'source': 'artifacts/adr_001_database_choice.md'}, page_content='## Context\\n- The project involves developing a new hire onboarding tool that requires a semantic search feature. The decision to use PostgreSQL with the `pgvector` extension over specialized vector databases such as ChromaDB, FAISS, or Weaviate involves several considerations.\\n- PostgreSQL is a well-established relational database system, familiar to many development teams, which can reduce the initial learning curve and integration time.\\n- Leveraging PostgreSQL with `pgvector` allows the use of existing database infrastructure, minimizing operational complexity by avoiding the need to manage a separate system for vector storage and search.\\n- Cost-effectiveness is another important factor, as integrating `pgvector` into a current PostgreSQL setup may incur fewer additional costs than adopting a new technology stack.'),\n",
       "  Document(id='2c0616e0-5177-4ebb-bd96-a91e90ccc75e', metadata={'source': 'artifacts/day1_prd.md'}, page_content='FutureWork(description=\"AI-powered personalized learning paths for new hires.\")\\n    ],\\n    appendix=[\\n        AppendixItem(type=\"Open Question\", description=\"Which team will be responsible for maintaining the content in the document repository?\"),\\n        AppendixItem(type=\"Dependency\", description=\"The final UI design mockups are required from the Design team by [Date].\")\\n    ]\\n)'),\n",
       "  Document(id='ad4bcd81-7f18-407a-86e7-fa1aa1da73e6', metadata={'source': 'artifacts/schema.sql'}, page_content=\"CREATE TABLE users (\\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\\n    name TEXT NOT NULL,\\n    email TEXT NOT NULL UNIQUE,\\n    role TEXT CHECK(role IN ('New Hire', 'Manager', 'HR Manager')) NOT NULL\\n);\\n\\nCREATE TABLE onboarding_tasks (\\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\\n    title TEXT NOT NULL,\\n    description TEXT,\\n    due_date DATE,\\n    status TEXT CHECK(status IN ('Pending', 'Completed')) NOT NULL DEFAULT 'Pending',\\n    user_id INTEGER,\\n    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\\n);\"),\n",
       "  Document(id='b31691e6-9045-4580-9bf9-ec4dab2691ae', metadata={'source': 'artifacts/adr_001_database_choice.md'}, page_content=\"- **Future Work:**\\n  - Monitor the system's performance and scalability as the dataset grows to determine if a transition to a specialized vector database becomes necessary.\\n  - Evaluate the need for potential architectural adjustments to accommodate high-dimensional vector data and improve search performance if required.\\n```\")],\n",
       " 'answer': 'The project is developing a new hire onboarding tool. This tool aims to help streamline and improve the onboarding process for new employees. Some key features and goals include:\\n\\n- Implementing a semantic search feature for the onboarding tool, which will be powered by PostgreSQL with the pgvector extension.\\n- Providing AI-powered personalized learning paths for new hires in the future.\\n- Maintaining a user system with different roles such as New Hire, Manager, and HR Manager, along with onboarding tasks that can be assigned and tracked for each user.\\n\\nThe choice of PostgreSQL with pgvector was made to take advantage of existing infrastructure, reduce complexity, and keep costs down, while still enabling semantic (vector) search capabilities.\\n\\nIn summary, the project is building a smart onboarding platform with semantic search and plans for further personalization and AI-driven features for new employees.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Write the code for the two-agent system with a Grader and conditional edges.\n",
    "def retriever_node(state):\n",
    "    \"\"\"\n",
    "    Retriever node for LangGraph RAG system.\n",
    "    Takes the state dict, uses the retriever to get relevant documents,\n",
    "    and updates the state with the retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving documents...\")\n",
    "    question = state[\"question\"]\n",
    "    # Use the retriever to get relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    # Update the state with the retrieved documents\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def generator_node(state):\n",
    "    \"\"\" Generator node for LangGraph RAG system.\n",
    "    Takes the state dict, uses the LLM to generate an answer based on the question and documents,\n",
    "    and updates the state with the generated answer.\n",
    "    \"\"\"\n",
    "    print(\"Generating answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents =state[\"documents\"] \n",
    "    prompt = f\"\"\"You are an AI assistant that answers questions based on the provided documents.\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    # Typed dictionary stuff\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "def grader_node(state):\n",
    "    \"\"\" Generator node for LangGraph RAG system.\n",
    "    Takes the state dict, uses the LLM to generate an answer based on the question and documents,\n",
    "    and updates the state with the generated answer.\n",
    "    \"\"\"\n",
    "    print(\"Grading the answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents =state[\"documents\"] \n",
    "    prompt = f\"\"\"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\":\"system\", \"content\": \"You are a grading AI that determines if the provided documents are sufficient to answer the question.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    graded_answer = response.choices[0].message.content.strip()\n",
    "    if graded_answer.lower() == \"yes\":\n",
    "        return {\"documents\": documents}\n",
    "    else:\n",
    "        return {\"documents\": []}\n",
    "    \n",
    "def rewrite_question(state):\n",
    "    \"\"\"Rewrite question node for LangGraph RAG system.\n",
    "    This node is used to rewrite the question if the Grader node returns 'no'.\n",
    "    \"\"\"\n",
    "    print(\"Rewriting question...\")\n",
    "    question = state[\"question\"]\n",
    "    prompt = f\"\"\"Rewrite the following question to make it more specific and clear:\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    rewritten_question = response.choices[0].message.content.strip()\n",
    "    return {\"question\": rewritten_question}\n",
    "\n",
    "def continue_node(state):\n",
    "    \"\"\"Continue node for LangGraph RAG system.\n",
    "    This node is used to continue the process if the Grader node returns 'yes'.\n",
    "    \"\"\"\n",
    "    print(\"Continuing to generate answer...\")\n",
    "    if state[\"documents\"]:\n",
    "        print(\"Documents are sufficient, proceeding to answer generation.\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"Documents are not sufficient, ending process.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "# Define the state graph for the RAG system\n",
    "print(\"Creating the state graph...\")\n",
    "graph = StateGraph(State)\n",
    "print(\"adding nodes to the graph...\")\n",
    "graph.add_node(\"RETRIEVE\", retriever_node)\n",
    "graph.add_node(\"GENERATE\", generator_node)\n",
    "graph.add_node(\"GRADER\", grader_node)\n",
    "graph.add_node(\"REWRITE\", rewrite_question)\n",
    "\n",
    "\n",
    "print(\"adding edges to the graph...\")\n",
    "graph.set_entry_point(\"RETRIEVE\")\n",
    "graph.add_edge(\"RETRIEVE\", \"GRADER\")\n",
    "graph.add_conditional_edges(\n",
    "    \"GRADER\",\n",
    "    continue_node,\n",
    "    {\n",
    "        \"rewrite\": \"REWRITE\",\n",
    "        \"generate\": \"GENERATE\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"REWRITE\", \"RETRIEVE\")\n",
    "graph.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "app.invoke(State(question=\"What project do?\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Task:** Build a sophisticated \"research team\" of specialized agents that includes a router to delegate tasks to the correct specialist.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Specialize your retriever:** Create two separate retrievers. One for the PRD (`prd_retriever`) and one for the technical documents (`tech_retriever` for schema and ADRs).\n",
    "2.  **Define the Agents:**\n",
    "    * `ProjectManagerAgent`: This will be the entry point and will act as a router. It uses an LLM to decide whether the user's question is about product requirements or technical details, and routes to the appropriate researcher.\n",
    "    * `PRDResearcherAgent`: A node that uses the `prd_retriever`.\n",
    "    * `TechResearcherAgent`: A node that uses the `tech_retriever`.\n",
    "    * `SynthesizerAgent`: A node that takes the collected documents from either researcher and synthesizes a final answer.\n",
    "3.  **Build the Graph:** Use conditional edges to orchestrate the flow: The entry point is the `ProjectManager`, which then routes to either the `PRD_RESEARCHER` or `TECH_RESEARCHER`. Both of those nodes should then route to the `SYNTHESIZE` node, which then goes to the `END`.\n",
    "\n",
    "**Expected Quality:** A highly advanced agentic system that mimics a real-world research workflow, including a router and specialist roles, to improve the accuracy and efficiency of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 7 document splits...\n",
      "Creating vector store from 6 document splits...\n",
      "Creating the multi-agent RAG state graph...\n",
      "Routing to PRDResearcherAgent...\n",
      "Retrieving PRD documents...\n",
      "Synthesizing final answer...\n",
      "Final synthesized answer: The product requirements, synthesized from the provided documents, are as follows:\n",
      "\n",
      "**1. Executive Summary and Vision**\n",
      "- The product aims to streamline onboarding for new hires by providing a centralized, user-friendly platform.\n",
      "- Vision: Enhance productivity and engagement for new hires by reducing onboarding friction.\n",
      "\n",
      "**2. Problem Statement**\n",
      "- New hires currently face a fragmented and overwhelming onboarding experience.\n",
      "\n",
      "**3. User Personas and Scenarios**\n",
      "- The New Hire: Faces confusion with multiple onboarding platforms.\n",
      "- The Hiring Manager: Spends excessive time answering repetitive questions.\n",
      "- The HR Coordinator: Handles high volumes of support tickets.\n",
      "\n",
      "**4. Goals and Metrics**\n",
      "- (Specific metrics/goals are referenced, but not detailed in the excerpt.)\n",
      "\n",
      "**5. User Stories (Functional Requirements Example)**\n",
      "- Example: \"Given I am on the login page, when I enter invalid credentials, then I see a clear error message.\"\n",
      "- Other functional requirements likely include user login, onboarding task checklists, and document repositories (as inferred from milestone features).\n",
      "\n",
      "**6. Non-Functional Requirements**\n",
      "- **Performance:** Application must load in under 3 seconds on a standard corporate network.\n",
      "- **Security:** All data must be encrypted in transit and at rest; system must comply with company SSO (Single Sign-On) policies.\n",
      "- **Accessibility:** User interface must be compliant with WCAG 2.1 AA standards.\n",
      "- **Scalability:** System must support up to 500 concurrent users during peak onboarding seasons.\n",
      "\n",
      "**7. Release Plan (Features for MVP Version 1.0)**\n",
      "- Core features include:\n",
      "  - User login.\n",
      "  - Task checklist for onboarding.\n",
      "  - Document repository.\n",
      "\n",
      "**8. Out of Scope / Future Considerations**\n",
      "- Out of Scope: Not detailed, but out-of-scope and future work sections are present in the PRD template.\n",
      "- Future Work Example: \"AI-powered personalized learning paths for new hires.\"\n",
      "\n",
      "**9. Appendix (Dependencies and Open Questions)**\n",
      "- Open question: Which team will be responsible for maintaining content in the document repository?\n",
      "- Dependency: Final UI design mockups from the Design team by a specified date.\n",
      "\n",
      "---\n",
      "\n",
      "**In Summary:**  \n",
      "The core product requirements include a secure, accessible, and scalable onboarding platform featuring centralized login, onboarding checklists, and document storage, with performance and security benchmarks, designed to simplify the onboarding experience for new hires, hiring managers, and HR coordinators. Key future and dependency considerations are also noted.\n",
      "{'question': 'What are the product requirements?', 'documents': [Document(id='a1af6396-e263-4797-aadd-eb1b07cf0dfe', metadata={'source': 'artifacts/day1_prd.md'}, page_content='# Example usage:\\nprd = ProductRequirementsDocument(\\n    title=\"Product Requirements Document: Example Product\",\\n    status=\"Draft\",\\n    author=\"Team Example\",\\n    version=\"1.0\",\\n    last_updated=date(2023, 10, 5),\\n    executive_summary=\"This product aims to streamline onboarding for new hires by providing a centralized, user-friendly platform.\",\\n    vision=\"To enhance productivity and engagement for new hires by reducing onboarding friction.\",\\n    problem_statement=\"New hires currently face a fragmented and overwhelming onboarding experience...\",\\n    user_personas_scenarios=[\\n        PersonaScenario(persona=\"The New Hire\", scenario=\"Faces confusion with multiple onboarding platforms.\"),\\n        PersonaScenario(persona=\"The Hiring Manager\", scenario=\"Spends excessive time answering repetitive questions.\"),\\n        PersonaScenario(persona=\"The HR Coordinator\", scenario=\"Handles high volume of support tickets.\")\\n    ],\\n    goals_and_metrics=['), Document(id='73eeb1b2-4662-4cca-9665-d18d960dbf44', metadata={'source': 'artifacts/day1_prd.md'}, page_content='\"Given I am on the login page, when I enter invalid credentials, then I see a clear error message.\"\\n            ]\\n        )\\n    ],\\n    non_functional_requirements=[\\n        NonFunctionalRequirement(category=\"Performance\", description=\"The application must load in under 3 seconds on a standard corporate network connection.\"),\\n        NonFunctionalRequirement(category=\"Security\", description=\"All data must be encrypted in transit and at rest. The system must comply with company SSO policies.\"),\\n        NonFunctionalRequirement(category=\"Accessibility\", description=\"The user interface must be compliant with WCAG 2.1 AA standards.\"),\\n        NonFunctionalRequirement(category=\"Scalability\", description=\"The system must support up to 500 concurrent users during peak onboarding seasons.\")\\n    ],\\n    release_plan=[\\n        Milestone(version=\"1.0 (MVP)\", target_date=date(2023, 12, 1), features=[\"Core features including user login\", \"task checklist\", \"document repository\"]),'), Document(id='91752edd-54c3-429f-8836-806feee305e2', metadata={'source': 'artifacts/day1_prd.md'}, page_content='class OutOfScope(BaseModel):\\n    description: str\\n\\nclass FutureWork(BaseModel):\\n    description: str\\n\\nclass AppendixItem(BaseModel):\\n    type: str\\n    description: str\\n\\nclass ProductRequirementsDocument(BaseModel):\\n    title: str\\n    status: str\\n    author: str\\n    version: str\\n    last_updated: Optional[date]\\n    executive_summary: str\\n    vision: str\\n    problem_statement: str\\n    user_personas_scenarios: List[PersonaScenario]\\n    goals_and_metrics: List[Goal]\\n    user_stories: List[UserStory]\\n    non_functional_requirements: List[NonFunctionalRequirement]\\n    release_plan: List[Milestone]\\n    out_of_scope: List[OutOfScope]\\n    future_considerations: List[FutureWork]\\n    appendix: List[AppendixItem]'), Document(id='5938e922-ef35-4f8e-8fad-8133c56e5d4e', metadata={'source': 'artifacts/day1_prd.md'}, page_content='FutureWork(description=\"AI-powered personalized learning paths for new hires.\")\\n    ],\\n    appendix=[\\n        AppendixItem(type=\"Open Question\", description=\"Which team will be responsible for maintaining the content in the document repository?\"),\\n        AppendixItem(type=\"Dependency\", description=\"The final UI design mockups are required from the Design team by [Date].\")\\n    ]\\n)')], 'answer': 'The product requirements, synthesized from the provided documents, are as follows:\\n\\n**1. Executive Summary and Vision**\\n- The product aims to streamline onboarding for new hires by providing a centralized, user-friendly platform.\\n- Vision: Enhance productivity and engagement for new hires by reducing onboarding friction.\\n\\n**2. Problem Statement**\\n- New hires currently face a fragmented and overwhelming onboarding experience.\\n\\n**3. User Personas and Scenarios**\\n- The New Hire: Faces confusion with multiple onboarding platforms.\\n- The Hiring Manager: Spends excessive time answering repetitive questions.\\n- The HR Coordinator: Handles high volumes of support tickets.\\n\\n**4. Goals and Metrics**\\n- (Specific metrics/goals are referenced, but not detailed in the excerpt.)\\n\\n**5. User Stories (Functional Requirements Example)**\\n- Example: \"Given I am on the login page, when I enter invalid credentials, then I see a clear error message.\"\\n- Other functional requirements likely include user login, onboarding task checklists, and document repositories (as inferred from milestone features).\\n\\n**6. Non-Functional Requirements**\\n- **Performance:** Application must load in under 3 seconds on a standard corporate network.\\n- **Security:** All data must be encrypted in transit and at rest; system must comply with company SSO (Single Sign-On) policies.\\n- **Accessibility:** User interface must be compliant with WCAG 2.1 AA standards.\\n- **Scalability:** System must support up to 500 concurrent users during peak onboarding seasons.\\n\\n**7. Release Plan (Features for MVP Version 1.0)**\\n- Core features include:\\n  - User login.\\n  - Task checklist for onboarding.\\n  - Document repository.\\n\\n**8. Out of Scope / Future Considerations**\\n- Out of Scope: Not detailed, but out-of-scope and future work sections are present in the PRD template.\\n- Future Work Example: \"AI-powered personalized learning paths for new hires.\"\\n\\n**9. Appendix (Dependencies and Open Questions)**\\n- Open question: Which team will be responsible for maintaining content in the document repository?\\n- Dependency: Final UI design mockups from the Design team by a specified date.\\n\\n---\\n\\n**In Summary:**  \\nThe core product requirements include a secure, accessible, and scalable onboarding platform featuring centralized login, onboarding checklists, and document storage, with performance and security benchmarks, designed to simplify the onboarding experience for new hires, hiring managers, and HR coordinators. Key future and dependency considerations are also noted.', 'route': 'PRD_RESEARCHER'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and State\n",
    "class State(TypedDict):\n",
    "    \"\"\"State for the RAG system.\"\"\"\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    route: str \n",
    "\n",
    "prd_artifact_paths = [\"artifacts/day1_prd.md\"]\n",
    "tech_artifact_paths = [\"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "\n",
    "prd_retriever = create_knowledge_base(prd_artifact_paths)\n",
    "tech_retriever = create_knowledge_base(tech_artifact_paths)\n",
    "\n",
    "# 2. Define agent nodes\n",
    "def project_manager_node(state):\n",
    "    \"\"\"\n",
    "    Router node that decides which researcher to route to based on the question.\n",
    "    Uses simple keyword logic; can be replaced with an LLM for more advanced routing.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    if \"requirement\" in question.lower() or \"prd\" in question.lower():\n",
    "        print(\"Routing to PRDResearcherAgent...\")\n",
    "        return {\"route\": \"PRD_RESEARCHER\"}\n",
    "    else:\n",
    "        print(\"Routing to TechResearcherAgent...\")\n",
    "        return {\"route\": \"TECH_RESEARCHER\"}\n",
    "\n",
    "\n",
    "def prd_researcher_node(state):\n",
    "    \"\"\"\n",
    "    Uses prd_retriever to fetch relevant PRD documents for the question.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving PRD documents...\")\n",
    "    question = state[\"question\"]\n",
    "    prd_docs = prd_retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": prd_docs}\n",
    "\n",
    "\n",
    "def tech_researcher_node(state):\n",
    "    \"\"\"\n",
    "    Uses tech_retriever to fetch relevant technical documents for the question.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving technical documents...\")\n",
    "    question = state[\"question\"]\n",
    "    tech_docs = tech_retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": tech_docs}\n",
    "\n",
    "\n",
    "def synthesizer_node(state):\n",
    "    \"\"\"\n",
    "    Synthesizes a final answer from the documents retrieved by either researcher.\n",
    "    Calls the LLM with a prompt containing the question and the retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"Synthesizing final answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are an AI assistant that synthesizes answers from the following documents.\\nQuestion: {question}\\nDocuments: {documents}\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    print(f\"Final synthesized answer: {answer}\")\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# 3. Build the graph\n",
    "print(\"Creating the multi-agent RAG state graph...\")\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"PROJECT_MANAGER\", project_manager_node)\n",
    "graph.add_node(\"PRD_RESEARCHER\", prd_researcher_node)\n",
    "graph.add_node(\"TECH_RESEARCHER\", tech_researcher_node)\n",
    "graph.add_node(\"SYNTHESIZE\", synthesizer_node)\n",
    "\n",
    "graph.set_entry_point(\"PROJECT_MANAGER\")\n",
    "graph.add_conditional_edges(\n",
    "    \"PROJECT_MANAGER\",\n",
    "    lambda state: state[\"route\"],\n",
    "    {\n",
    "        \"PRD_RESEARCHER\": \"PRD_RESEARCHER\",\n",
    "        \"TECH_RESEARCHER\": \"TECH_RESEARCHER\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZE\")\n",
    "graph.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZE\")\n",
    "graph.add_edge(\"SYNTHESIZE\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# Example invocation\n",
    "result = app.invoke(State(question=\"What are the product requirements?\"))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
